{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Openpose.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPyZuw85jaUfwcFaei7pArM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomohitom/Tennis_Game_Analysis/blob/master/Openpose.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_l0MJA_sRmU",
        "outputId": "399ab81b-21b2-44f3-87f4-a820144198e0"
      },
      "source": [
        "!git clone \"https://github.com/YutaroOgawa/pytorch_advanced.git\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch_advanced'...\n",
            "remote: Enumerating objects: 529, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 529 (delta 23), reused 32 (delta 13), pack-reused 479\u001b[K\n",
            "Receiving objects: 100% (529/529), 17.57 MiB | 29.94 MiB/s, done.\n",
            "Resolving deltas: 100% (282/282), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig9q8g2lsma-"
      },
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tarfile\n",
        "\n",
        "data_dir = \"/content/pytorch_advanced/4_pose_estimation/data/\"\n",
        "if not os.path.exists(data_dir):\n",
        "    os.mkdir(data_dir)\n",
        "\n",
        "weights_dir = \"/content/pytorch_advanced/4_pose_estimation/weights/\"\n",
        "if not os.path.exists(weights_dir):\n",
        "    os.mkdir(weights_dir)\n",
        "\n",
        "url =  \"http://images.cocodataset.org/zips/val2014.zip\"\n",
        "target_path = os.path.join(data_dir, \"val2014.zip\") \n",
        "\n",
        "if not os.path.exists(target_path):\n",
        "    urllib.request.urlretrieve(url, target_path)\n",
        "    \n",
        "    zip = zipfile.ZipFile(target_path)\n",
        "    zip.extractall(data_dir)  # ZIPを解凍\n",
        "    zip.close()  # ZIPファイルをクローズ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1APQzpO3OOa"
      },
      "source": [
        "COCO.jsonとmask.tar.gzを「data」ディレクトリにアップロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeDeGx8d3DwK"
      },
      "source": [
        "save_path = os.path.join(\"/content/pytorch_advanced/4_pose_estimation/data\", \"mask.tar.gz\") \n",
        "\n",
        "with tarfile.open(save_path, 'r:*') as tar:\n",
        "    tar.extractall(\"/content/pytorch_advanced/4_pose_estimation/data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JZKn-I8KD5i"
      },
      "source": [
        "画像、マスク画像、アノテーションデータへのファイルパスリストを作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIch5jqiJiSn"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch.utils.data as data\n",
        "\n",
        "def make_datapath_list(rootpath):\n",
        "    json_path = os.path.join(rootpath, \"COCO.json\")\n",
        "    with open(json_path) as data_file:\n",
        "        data_this = json.load(data_file)\n",
        "        data_json = data_this[\"root\"]\n",
        "\n",
        "    num_samples = len(data_json)\n",
        "    train_indexes = []\n",
        "    val_indexes = []\n",
        "    for count in range(num_samples):\n",
        "        if data_json[count][\"isValidation\"] != 0.:\n",
        "            val_indexes.append(count)\n",
        "        else:\n",
        "            train_indexes.append(count)\n",
        "\n",
        "    train_img_list = list()\n",
        "    val_img_list = list()\n",
        "\n",
        "    for idx in train_indexes:\n",
        "        img_path = os.path.join(rootpath, data_json[idx][\"img_paths\"])\n",
        "        train_img_list.append(img_path)\n",
        "\n",
        "    for idx in val_indexes:\n",
        "        img_path = os.path.join(rootpath, data_json[idx][\"img_paths\"])\n",
        "        val_img_list.append(img_path)\n",
        "\n",
        "    train_mask_list = []\n",
        "    val_mask_list = []\n",
        "\n",
        "    for idx in train_indexes:\n",
        "        img_idx = data_json[idx]['img_paths'][-16:-4]\n",
        "        anno_path = \"./data/mask/train2014/mask_COCO_tarin2014_\" + img_idx+'.jpg'\n",
        "        train_mask_list.append(anno_path)\n",
        "\n",
        "    for idx in val_indexes:\n",
        "        img_idx = data_json[idx]['img_paths'][-16:-4]\n",
        "        anno_path = \"./data/mask/val2014/mask_COCO_val2014_\" + img_idx+'.jpg'\n",
        "        val_mask_list.append(anno_path)\n",
        "\n",
        "    train_meta_list = list()\n",
        "    val_meta_list = list()\n",
        "\n",
        "    for idx in train_indexes:\n",
        "        train_meta_list.append(data_json[idx])\n",
        "\n",
        "    for idx in val_indexes:\n",
        "        val_meta_list.append(data_json[idx])\n",
        "\n",
        "    return train_img_list, train_mask_list, val_img_list, val_mask_list, train_meta_list, val_meta_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMIvxJz0JsJE"
      },
      "source": [
        "# 動作確認（実行には10秒ほど時間がかかります）\n",
        "train_img_list, train_mask_list, val_img_list, val_mask_list, train_meta_list, val_meta_list = make_datapath_list(\n",
        "    rootpath=\"./data/\")\n",
        "\n",
        "val_meta_list[24]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR6EHxypJ5B9"
      },
      "source": [
        "index = 24\n",
        "\n",
        "img = cv2.imread(val_img_list[index])\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "mask_miss = cv2.imread(val_mask_list[index])\n",
        "mask_miss = cv2.cvtColor(mask_miss, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(mask_miss)\n",
        "plt.show()\n",
        "\n",
        "blend_img = cv2.addWeighted(img, 0.4, mask_miss, 0.6, 0)\n",
        "plt.imshow(blend_img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geriDtKrKQ6C"
      },
      "source": [
        "画像の前処理作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQwPJE25KUxg"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "class Compose(object):\n",
        "    \"\"\"引数transformに格納された変形を順番に実行するクラス\n",
        "       対象画像、マスク画像、アノテーション画像を同時に変換させます。 \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, meta_data, img, mask_miss):\n",
        "        for t in self.transforms:\n",
        "            meta_data, img, mask_miss = t(meta_data, img, mask_miss)\n",
        "\n",
        "        return meta_data, img, mask_miss\n",
        "\n",
        "\n",
        "class get_anno(object):\n",
        "    \"\"\"JSON形式のアノテーションデータを辞書オブジェクトに格納\"\"\"\n",
        "\n",
        "    def __call__(self, meta_data, img, mask_miss):\n",
        "        anno = dict()\n",
        "        anno['dataset'] = meta_data['dataset']\n",
        "        anno['img_height'] = int(meta_data['img_height'])\n",
        "        anno['img_width'] = int(meta_data['img_width'])\n",
        "\n",
        "        anno['isValidation'] = meta_data['isValidation']\n",
        "        anno['people_index'] = int(meta_data['people_index'])\n",
        "        anno['annolist_index'] = int(meta_data['annolist_index'])\n",
        "\n",
        "        # (b) objpos_x (float), objpos_y (float)\n",
        "        anno['objpos'] = np.array(meta_data['objpos'])\n",
        "        anno['scale_provided'] = meta_data['scale_provided']\n",
        "        anno['joint_self'] = np.array(meta_data['joint_self'])\n",
        "\n",
        "        anno['numOtherPeople'] = int(meta_data['numOtherPeople'])\n",
        "        anno['num_keypoints_other'] = np.array(\n",
        "            meta_data['num_keypoints_other'])\n",
        "        anno['joint_others'] = np.array(meta_data['joint_others'])\n",
        "        anno['objpos_other'] = np.array(meta_data['objpos_other'])\n",
        "        anno['scale_provided_other'] = meta_data['scale_provided_other']\n",
        "        anno['bbox_other'] = meta_data['bbox_other']\n",
        "        anno['segment_area_other'] = meta_data['segment_area_other']\n",
        "\n",
        "        if anno['numOtherPeople'] == 1:\n",
        "            anno['joint_others'] = np.expand_dims(anno['joint_others'], 0)\n",
        "            anno['objpos_other'] = np.expand_dims(anno['objpos_other'], 0)\n",
        "\n",
        "        meta_data = anno\n",
        "\n",
        "        return meta_data, img, mask_miss\n",
        "\n",
        "\n",
        "class add_neck(object):\n",
        "    '''\n",
        "    アノテーションデータの順番を変更し、さらに首のアノテーションデータを追加します。\n",
        "    首ポジションは両肩の位置から計算します。\n",
        "\n",
        "    MS COCO annotation order:\n",
        "    0: nose\t   \t\t1: l eye\t\t2: r eye\t3: l ear\t4: r ear\n",
        "    5: l shoulder\t6: r shoulder\t7: l elbow\t8: r elbow\n",
        "    9: l wrist\t\t10: r wrist\t\t11: l hip\t12: r hip\t13: l knee\n",
        "    14: r knee\t\t15: l ankle\t\t16: r ankle\n",
        "    The order in this work:\n",
        "    (0-'nose'\t1-'neck' 2-'right_shoulder' 3-'right_elbow' 4-'right_wrist'\n",
        "    5-'left_shoulder' 6-'left_elbow'\t    7-'left_wrist'  8-'right_hip'\n",
        "    9-'right_knee'\t 10-'right_ankle'\t11-'left_hip'   12-'left_knee'\n",
        "    13-'left_ankle'\t 14-'right_eye'\t    15-'left_eye'   16-'right_ear'\n",
        "    17-'left_ear' )\n",
        "    '''\n",
        "\n",
        "    def __call__(self, meta_data, img, mask_miss):\n",
        "        meta = meta_data\n",
        "        our_order = [0, 17, 6, 8, 10, 5, 7, 9,\n",
        "                     12, 14, 16, 11, 13, 15, 2, 1, 4, 3]\n",
        "        # Index 6 is right shoulder and Index 5 is left shoulder\n",
        "        right_shoulder = meta['joint_self'][6, :]\n",
        "        left_shoulder = meta['joint_self'][5, :]\n",
        "        neck = (right_shoulder + left_shoulder) / 2\n",
        "\n",
        "        # right_shoulder[2]が値1のときはアノテーションがあり画像内に部位も見えている\n",
        "        # 値0のときはアノテーションの座標情報はあるが、画像内に部位は映っていない\n",
        "        # 値が2のときは画像内に写っておらず、アノテーション付けもない\n",
        "        # ※注意　元のMSCOCOの定義と値の意味が変わっている\n",
        "        # v=0: not labeled (in which case x=y=0), v=1: labeled but not visible, and v=2: labeled and visible.\n",
        "        if right_shoulder[2] == 2 or left_shoulder[2] == 2:\n",
        "            neck[2] = 2\n",
        "        elif right_shoulder[2] == 1 or left_shoulder[2] == 1:\n",
        "            neck[2] = 1\n",
        "        else:\n",
        "            neck[2] = right_shoulder[2] * left_shoulder[2]\n",
        "\n",
        "        neck = neck.reshape(1, len(neck))\n",
        "        neck = np.round(neck)\n",
        "        meta['joint_self'] = np.vstack((meta['joint_self'], neck))\n",
        "        meta['joint_self'] = meta['joint_self'][our_order, :]\n",
        "        temp = []\n",
        "\n",
        "        for i in range(meta['numOtherPeople']):\n",
        "            right_shoulder = meta['joint_others'][i, 6, :]\n",
        "            left_shoulder = meta['joint_others'][i, 5, :]\n",
        "            neck = (right_shoulder + left_shoulder) / 2\n",
        "            if (right_shoulder[2] == 2 or left_shoulder[2] == 2):\n",
        "                neck[2] = 2\n",
        "            elif (right_shoulder[2] == 1 or left_shoulder[2] == 1):\n",
        "                neck[2] = 1\n",
        "            else:\n",
        "                neck[2] = right_shoulder[2] * left_shoulder[2]\n",
        "            neck = neck.reshape(1, len(neck))\n",
        "            neck = np.round(neck)\n",
        "            single_p = np.vstack((meta['joint_others'][i], neck))\n",
        "            single_p = single_p[our_order, :]\n",
        "            temp.append(single_p)\n",
        "        meta['joint_others'] = np.array(temp)\n",
        "\n",
        "        meta_data = meta\n",
        "\n",
        "        return meta_data, img, mask_miss\n",
        "\n",
        "\n",
        "class aug_scale(object):\n",
        "    def __init__(self):\n",
        "        self.params_transform = dict()\n",
        "        self.params_transform['scale_min'] = 0.5\n",
        "        self.params_transform['scale_max'] = 1.1\n",
        "        self.params_transform['target_dist'] = 0.6\n",
        "\n",
        "    def __call__(self, meta_data, img, mask_miss):\n",
        "\n",
        "        # ランダムに0.5倍～1.1倍する\n",
        "        dice = random.random()  # (0,1)\n",
        "        scale_multiplier = (\n",
        "            self.params_transform['scale_max'] - self.params_transform['scale_min']) * dice + self.params_transform['scale_min']\n",
        "\n",
        "        scale_abs = self.params_transform['target_dist'] / \\\n",
        "            meta_data['scale_provided']\n",
        "        scale = scale_abs * scale_multiplier\n",
        "        img = cv2.resize(img, (0, 0), fx=scale, fy=scale,\n",
        "                         interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "        mask_miss = cv2.resize(mask_miss, (0, 0), fx=scale,\n",
        "                               fy=scale, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "        # modify meta data\n",
        "        meta_data['objpos'] *= scale\n",
        "        meta_data['joint_self'][:, :2] *= scale\n",
        "        if (meta_data['numOtherPeople'] != 0):\n",
        "            meta_data['objpos_other'] *= scale\n",
        "            meta_data['joint_others'][:, :, :2] *= scale\n",
        "        return meta_data, img, mask_miss\n",
        "\n",
        "\n",
        "class aug_rotate(object):\n",
        "    def __init__(self):\n",
        "        self.params_transform = dict()\n",
        "        self.params_transform['max_rotate_degree'] = 40\n",
        "\n",
        "    def __call__(self, meta_data, img, mask_miss):\n",
        "\n",
        "        # ランダムに-40～40度回転\n",
        "        dice = random.random()  # (0,1)\n",
        "        degree = (dice - 0.5) * 2 * \\\n",
        "            self.params_transform['max_rotate_degree']  # degree [-40,40]\n",
        "\n",
        "        def rotate_bound(image, angle, bordervalue):\n",
        "            # grab the dimensions of the image and then determine the\n",
        "            # center\n",
        "            (h, w) = image.shape[:2]\n",
        "            (cX, cY) = (w // 2, h // 2)\n",
        "\n",
        "            # grab the rotation matrix (applying the negative of the\n",
        "            # angle to rotate clockwise), then grab the sine and cosine\n",
        "            # (i.e., the rotation components of the matrix)\n",
        "            M = cv2.getRotationMatrix2D((cX, cY), -angle, 1.0)\n",
        "            cos = np.abs(M[0, 0])\n",
        "            sin = np.abs(M[0, 1])\n",
        "\n",
        "            # compute the new bounding dimensions of the image\n",
        "            nW = int((h * sin) + (w * cos))\n",
        "            nH = int((h * cos) + (w * sin))\n",
        "\n",
        "            # adjust the rotation matrix to take into account translation\n",
        "            M[0, 2] += (nW / 2) - cX\n",
        "            M[1, 2] += (nH / 2) - cY\n",
        "\n",
        "            # perform the actual rotation and return the image\n",
        "            return cv2.warpAffine(image, M, (nW, nH), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_CONSTANT,\n",
        "                                  borderValue=bordervalue), M\n",
        "\n",
        "        def rotatepoint(p, R):\n",
        "            point = np.zeros((3, 1))\n",
        "            point[0] = p[0]\n",
        "            point[1] = p[1]\n",
        "            point[2] = 1\n",
        "\n",
        "            new_point = R.dot(point)\n",
        "\n",
        "            p[0] = new_point[0]\n",
        "\n",
        "            p[1] = new_point[1]\n",
        "            return p\n",
        "\n",
        "        # 画像とマスク画像の回転\n",
        "        img_rot, R = rotate_bound(img, np.copy(\n",
        "            degree), (128, 128, 128))  # 回転でできた隙間は青色\n",
        "        mask_miss_rot, _ = rotate_bound(\n",
        "            mask_miss, np.copy(degree), (255, 255, 255))\n",
        "\n",
        "        # アノテーションデータの回転\n",
        "        meta_data['objpos'] = rotatepoint(meta_data['objpos'], R)\n",
        "\n",
        "        for i in range(18):\n",
        "            meta_data['joint_self'][i, :] = rotatepoint(\n",
        "                meta_data['joint_self'][i, :], R)\n",
        "\n",
        "        for j in range(meta_data['numOtherPeople']):\n",
        "\n",
        "            meta_data['objpos_other'][j, :] = rotatepoint(\n",
        "                meta_data['objpos_other'][j, :], R)\n",
        "\n",
        "            for i in range(18):\n",
        "                meta_data['joint_others'][j, i, :] = rotatepoint(\n",
        "                    meta_data['joint_others'][j, i, :], R)\n",
        "\n",
        "        return meta_data, img_rot, mask_miss_rot\n",
        "\n",
        "\n",
        "class aug_croppad(object):\n",
        "    def __init__(self):\n",
        "        self.params_transform = dict()\n",
        "        self.params_transform['center_perterb_max'] = 40\n",
        "        self.params_transform['crop_size_x'] = 368\n",
        "        self.params_transform['crop_size_y'] = 368\n",
        "\n",
        "    def __call__(self, meta_data, img, mask_miss):\n",
        "\n",
        "        # ランダムにオフセットを用意 -40から40\n",
        "        dice_x = random.random()  # (0,1)\n",
        "        dice_y = random.random()  # (0,1)\n",
        "        crop_x = int(self.params_transform['crop_size_x'])\n",
        "        crop_y = int(self.params_transform['crop_size_y'])\n",
        "        x_offset = int((dice_x - 0.5) * 2 *\n",
        "                       self.params_transform['center_perterb_max'])\n",
        "        y_offset = int((dice_y - 0.5) * 2 *\n",
        "                       self.params_transform['center_perterb_max'])\n",
        "\n",
        "        center = meta_data['objpos'] + np.array([x_offset, y_offset])\n",
        "        center = center.astype(int)\n",
        "\n",
        "        # pad up and down\n",
        "        # img.shape=（幅、高さ）\n",
        "        pad_v = np.ones((crop_y, img.shape[1], 3), dtype=np.uint8) * 128\n",
        "        pad_v_mask_miss = np.ones(\n",
        "            (crop_y, mask_miss.shape[1], 3), dtype=np.uint8) * 255\n",
        "        img = np.concatenate((pad_v, img, pad_v), axis=0)\n",
        "\n",
        "        mask_miss = np.concatenate(\n",
        "            (pad_v_mask_miss, mask_miss, pad_v_mask_miss), axis=0)\n",
        "\n",
        "        # pad right and left\n",
        "        # img.shape=（幅、高さ）\n",
        "        pad_h = np.ones((img.shape[0], crop_x, 3), dtype=np.uint8) * 128\n",
        "        pad_h_mask_miss = np.ones(\n",
        "            (mask_miss.shape[0], crop_x, 3), dtype=np.uint8) * 255\n",
        "\n",
        "        img = np.concatenate((pad_h, img, pad_h), axis=1)\n",
        "        mask_miss = np.concatenate(\n",
        "            (pad_h_mask_miss, mask_miss, pad_h_mask_miss), axis=1)\n",
        "\n",
        "        # 切り出す\n",
        "        img = img[int(center[1] + crop_y / 2):int(center[1] + crop_y / 2 + crop_y),\n",
        "                  int(center[0] + crop_x / 2):int(center[0] + crop_x / 2 + crop_x), :]\n",
        "\n",
        "        mask_miss = mask_miss[int(center[1] + crop_y / 2):int(center[1] + crop_y / 2 +\n",
        "                                                              crop_y + 1*0), int(center[0] + crop_x / 2):int(center[0] + crop_x / 2 + crop_x + 1*0)]\n",
        "\n",
        "        offset_left = crop_x / 2 - center[0]\n",
        "        offset_up = crop_y / 2 - center[1]\n",
        "\n",
        "        offset = np.array([offset_left, offset_up])\n",
        "        meta_data['objpos'] += offset\n",
        "        meta_data['joint_self'][:, :2] += offset\n",
        "\n",
        "        # 画像からはみ出ていないかチェック\n",
        "        # 条件式4つのORを計算する\n",
        "        mask = np.logical_or.reduce((meta_data['joint_self'][:, 0] >= crop_x,\n",
        "                                     meta_data['joint_self'][:, 0] < 0,\n",
        "                                     meta_data['joint_self'][:, 1] >= crop_y,\n",
        "                                     meta_data['joint_self'][:, 1] < 0))\n",
        "\n",
        "        meta_data['joint_self'][mask == True, 2] = 2\n",
        "        if (meta_data['numOtherPeople'] != 0):\n",
        "            meta_data['objpos_other'] += offset\n",
        "            meta_data['joint_others'][:, :, :2] += offset\n",
        "\n",
        "            # 条件式4つのORを計算する\n",
        "            mask = np.logical_or.reduce((meta_data['joint_others'][:, :, 0] >= crop_x,\n",
        "                                         meta_data['joint_others'][:,\n",
        "                                                                   :, 0] < 0,\n",
        "                                         meta_data['joint_others'][:,\n",
        "                                                                   :, 1] >= crop_y,\n",
        "                                         meta_data['joint_others'][:, :, 1] < 0))\n",
        "\n",
        "            meta_data['joint_others'][mask == True, 2] = 2\n",
        "\n",
        "        return meta_data, img, mask_miss\n",
        "\n",
        "\n",
        "class aug_flip(object):\n",
        "    def __init__(self):\n",
        "        self.params_transform = dict()\n",
        "        self.params_transform['flip_prob'] = 0.5\n",
        "\n",
        "    def __call__(self, meta_data, img, mask_miss):\n",
        "\n",
        "        # ランダムにオフセットを用意 -40から40\n",
        "\n",
        "        dice = random.random()  # (0,1)\n",
        "        doflip = dice <= self.params_transform['flip_prob']\n",
        "\n",
        "        if doflip:\n",
        "            img = img.copy()\n",
        "            cv2.flip(src=img, flipCode=1, dst=img)\n",
        "            w = img.shape[1]  # img.shape=（幅、高さ）\n",
        "\n",
        "            mask_miss = mask_miss.copy()\n",
        "            cv2.flip(src=mask_miss, flipCode=1, dst=mask_miss)\n",
        "\n",
        "            '''\n",
        "            The order in this work:\n",
        "                (0-'nose'   1-'neck' 2-'right_shoulder' 3-'right_elbow' 4-'right_wrist'\n",
        "                5-'left_shoulder' 6-'left_elbow'        7-'left_wrist'  8-'right_hip'  \n",
        "                9-'right_knee'   10-'right_ankle'   11-'left_hip'   12-'left_knee' \n",
        "                13-'left_ankle'  14-'right_eye'     15-'left_eye'   16-'right_ear' \n",
        "                17-'left_ear' )\n",
        "            '''\n",
        "            meta_data['objpos'][0] = w - 1 - meta_data['objpos'][0]\n",
        "            meta_data['joint_self'][:, 0] = w - \\\n",
        "                1 - meta_data['joint_self'][:, 0]\n",
        "            # print meta['joint_self']\n",
        "            meta_data['joint_self'] = meta_data['joint_self'][[0, 1, 5, 6,\n",
        "                                                               7, 2, 3, 4, 11, 12, 13, 8, 9, 10, 15, 14, 17, 16]]\n",
        "\n",
        "            num_other_people = meta_data['numOtherPeople']\n",
        "            if (num_other_people != 0):\n",
        "                meta_data['objpos_other'][:, 0] = w - \\\n",
        "                    1 - meta_data['objpos_other'][:, 0]\n",
        "                meta_data['joint_others'][:, :, 0] = w - \\\n",
        "                    1 - meta_data['joint_others'][:, :, 0]\n",
        "                for i in range(num_other_people):\n",
        "                    meta_data['joint_others'][i] = meta_data['joint_others'][i][[\n",
        "                        0, 1, 5, 6, 7, 2, 3, 4, 11, 12, 13, 8, 9, 10, 15, 14, 17, 16]]\n",
        "\n",
        "        return meta_data, img, mask_miss\n",
        "\n",
        "\n",
        "class remove_illegal_joint(object):\n",
        "    \"\"\"データオーギュメンテーションの結果、画像内から外れたパーツの位置情報を変更する\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.params_transform = dict()\n",
        "        self.params_transform['crop_size_x'] = 368\n",
        "        self.params_transform['crop_size_y'] = 368\n",
        "\n",
        "    def __call__(self, meta_data, img, mask_miss):\n",
        "        crop_x = int(self.params_transform['crop_size_x'])\n",
        "        crop_y = int(self.params_transform['crop_size_y'])\n",
        "\n",
        "        # 条件式4つのORを計算する\n",
        "        mask = np.logical_or.reduce((meta_data['joint_self'][:, 0] >= crop_x,\n",
        "                                     meta_data['joint_self'][:, 0] < 0,\n",
        "                                     meta_data['joint_self'][:, 1] >= crop_y,\n",
        "                                     meta_data['joint_self'][:, 1] < 0))\n",
        "\n",
        "        # 画像内の枠からはみ出たパーツの位置情報は(1,1,2)にする\n",
        "        meta_data['joint_self'][mask == True, :] = (1, 1, 2)\n",
        "\n",
        "        if (meta_data['numOtherPeople'] != 0):\n",
        "            mask = np.logical_or.reduce((meta_data['joint_others'][:, :, 0] >= crop_x,\n",
        "                                         meta_data['joint_others'][:,\n",
        "                                                                   :, 0] < 0,\n",
        "                                         meta_data['joint_others'][:,\n",
        "                                                                   :, 1] >= crop_y,\n",
        "                                         meta_data['joint_others'][:, :, 1] < 0))\n",
        "            meta_data['joint_others'][mask == True, :] = (1, 1, 2)\n",
        "\n",
        "        return meta_data, img, mask_miss\n",
        "\n",
        "\n",
        "class Normalize_Tensor(object):\n",
        "    def __init__(self):\n",
        "        self.color_mean = [0.485, 0.456, 0.406]\n",
        "        self.color_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    def __call__(self, meta_data, img, mask_miss):\n",
        "\n",
        "        # 画像の大きさは最大1に規格化される\n",
        "        img = img.astype(np.float32) / 255.\n",
        "\n",
        "        # 色情報の標準化\n",
        "        preprocessed_img = img.copy()[:, :, ::-1]  # BGR→RGB\n",
        "\n",
        "        for i in range(3):\n",
        "            preprocessed_img[:, :, i] = preprocessed_img[:,\n",
        "                                                         :, i] - self.color_mean[i]\n",
        "            preprocessed_img[:, :, i] = preprocessed_img[:,\n",
        "                                                         :, i] / self.color_std[i]\n",
        "\n",
        "        # （幅、高さ、色）→（色、幅、高さ）\n",
        "        img = preprocessed_img.transpose((2, 0, 1)).astype(np.float32)\n",
        "        mask_miss = mask_miss.transpose((2, 0, 1)).astype(np.float32)\n",
        "\n",
        "        # 画像をTensorに\n",
        "        img = torch.from_numpy(img)\n",
        "        mask_miss = torch.from_numpy(mask_miss)\n",
        "\n",
        "        return meta_data, img, mask_miss\n",
        "\n",
        "\n",
        "class no_Normalize_Tensor(object):\n",
        "    def __init__(self):\n",
        "        self.color_mean = [0, 0, 0]\n",
        "        self.color_std = [1, 1, 1]\n",
        "\n",
        "    def __call__(self, meta_data, img, mask_miss):\n",
        "\n",
        "        # 画像の大きさは最大1に規格化される\n",
        "        img = img.astype(np.float32) / 255.\n",
        "\n",
        "        # 色情報の標準化\n",
        "        preprocessed_img = img.copy()[:, :, ::-1]  # BGR→RGB\n",
        "\n",
        "        for i in range(3):\n",
        "            preprocessed_img[:, :, i] = preprocessed_img[:,\n",
        "                                                         :, i] - self.color_mean[i]\n",
        "            preprocessed_img[:, :, i] = preprocessed_img[:,\n",
        "                                                         :, i] / self.color_std[i]\n",
        "\n",
        "        # （幅、高さ、色）→（色、幅、高さ）\n",
        "        img = preprocessed_img.transpose((2, 0, 1)).astype(np.float32)\n",
        "        mask_miss = mask_miss.transpose((2, 0, 1)).astype(np.float32)\n",
        "\n",
        "        # 画像をTensorに\n",
        "        img = torch.from_numpy(img)\n",
        "        mask_miss = torch.from_numpy(mask_miss)\n",
        "\n",
        "        return meta_data, img, mask_miss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmQOatMBKBUB"
      },
      "source": [
        "class DataTransform():\n",
        "    def __init__(self):\n",
        "\n",
        "        self.data_transform = {\n",
        "            'train': Compose([\n",
        "                get_anno(),  # JSONからアノテーションを辞書に格納\n",
        "                add_neck(),  # アノテーションデータの順番を変更し、さらに首のアノテーションデータを追加\n",
        "                aug_scale(),  # 拡大縮小\n",
        "                aug_rotate(),  # 回転\n",
        "                aug_croppad(),  # 切り出し\n",
        "                aug_flip(),  # 左右反転\n",
        "                remove_illegal_joint(),  # 画像からはみ出たアノテーションを除去\n",
        "                # Normalize_Tensor()  # 色情報の標準化とテンソル化\n",
        "                no_Normalize_Tensor()  # 本節のみ、色情報の標準化をなくす\n",
        "            ]),\n",
        "            'val': Compose([\n",
        "                # 本書では検証は省略\n",
        "            ])\n",
        "        }\n",
        "\n",
        "    def __call__(self, phase, meta_data, img, mask_miss):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        phase : 'train' or 'val'\n",
        "            前処理のモードを指定。\n",
        "        \"\"\"\n",
        "        meta_data, img, mask_miss = self.data_transform[phase](\n",
        "            meta_data, img, mask_miss)\n",
        "\n",
        "        return meta_data, img, mask_miss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ouz62HYgO8Il"
      },
      "source": [
        "# 動作確認\n",
        "# 画像読み込み\n",
        "index = 24\n",
        "img = cv2.imread(val_img_list[index])\n",
        "mask_miss = cv2.imread(val_mask_list[index])\n",
        "meat_data = val_meta_list[index]\n",
        "\n",
        "# 画像前処理\n",
        "transform = DataTransform()\n",
        "meta_data, img, mask_miss = transform(\"train\", meat_data, img, mask_miss)\n",
        "\n",
        "# 画像表示\n",
        "img = img.numpy().transpose((1, 2, 0))\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "# マスク表示\n",
        "mask_miss = mask_miss.numpy().transpose((1, 2, 0))\n",
        "plt.imshow(mask_miss)\n",
        "plt.show()\n",
        "\n",
        "# 合成 RGBにそろえてから\n",
        "img = Image.fromarray(np.uint8(img*255))\n",
        "img = np.asarray(img.convert('RGB'))\n",
        "mask_miss = Image.fromarray(np.uint8((mask_miss)))\n",
        "mask_miss = np.asarray(mask_miss.convert('RGB'))\n",
        "blend_img = cv2.addWeighted(img, 0.4, mask_miss, 0.6, 0)\n",
        "plt.imshow(blend_img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k4tMLhZPFxV"
      },
      "source": [
        "\n",
        "訓練データの正解情報として使うアノテーションデータの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txjz7VYBO_5u"
      },
      "source": [
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from scipy import misc, ndimage\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "\n",
        "def putGaussianMaps(center, accumulate_confid_map, params_transform):\n",
        "    '''ガウスマップに変換する'''\n",
        "    crop_size_y = params_transform['crop_size_y']\n",
        "    crop_size_x = params_transform['crop_size_x']\n",
        "    stride = params_transform['stride']\n",
        "    sigma = params_transform['sigma']\n",
        "\n",
        "    grid_y = crop_size_y / stride\n",
        "    grid_x = crop_size_x / stride\n",
        "    start = stride / 2.0 - 0.5\n",
        "    y_range = [i for i in range(int(grid_y))]\n",
        "    x_range = [i for i in range(int(grid_x))]\n",
        "    xx, yy = np.meshgrid(x_range, y_range)\n",
        "    xx = xx * stride + start\n",
        "    yy = yy * stride + start\n",
        "    d2 = (xx - center[0]) ** 2 + (yy - center[1]) ** 2\n",
        "    exponent = d2 / 2.0 / sigma / sigma\n",
        "    mask = exponent <= 4.6052\n",
        "    cofid_map = np.exp(-exponent)\n",
        "    cofid_map = np.multiply(mask, cofid_map)\n",
        "    accumulate_confid_map += cofid_map\n",
        "    accumulate_confid_map[accumulate_confid_map > 1.0] = 1.0\n",
        "\n",
        "    return accumulate_confid_map\n",
        "\n",
        "\n",
        "def putVecMaps(centerA, centerB, accumulate_vec_map, count, params_transform):\n",
        "    '''Parts A Fieldのベクトルを求める'''\n",
        "\n",
        "    centerA = centerA.astype(float)\n",
        "    centerB = centerB.astype(float)\n",
        "\n",
        "    stride = params_transform['stride']\n",
        "    crop_size_y = params_transform['crop_size_y']\n",
        "    crop_size_x = params_transform['crop_size_x']\n",
        "    grid_y = crop_size_y / stride\n",
        "    grid_x = crop_size_x / stride\n",
        "    thre = params_transform['limb_width']   # limb width\n",
        "    centerB = centerB / stride\n",
        "    centerA = centerA / stride\n",
        "\n",
        "    limb_vec = centerB - centerA\n",
        "    norm = np.linalg.norm(limb_vec)\n",
        "    if (norm == 0.0):\n",
        "        # print 'limb is too short, ignore it...'\n",
        "        return accumulate_vec_map, count\n",
        "    limb_vec_unit = limb_vec / norm\n",
        "    # print 'limb unit vector: {}'.format(limb_vec_unit)\n",
        "\n",
        "    # To make sure not beyond the border of this two points\n",
        "    min_x = max(int(round(min(centerA[0], centerB[0]) - thre)), 0)\n",
        "    max_x = min(int(round(max(centerA[0], centerB[0]) + thre)), grid_x)\n",
        "    min_y = max(int(round(min(centerA[1], centerB[1]) - thre)), 0)\n",
        "    max_y = min(int(round(max(centerA[1], centerB[1]) + thre)), grid_y)\n",
        "\n",
        "    range_x = list(range(int(min_x), int(max_x), 1))\n",
        "    range_y = list(range(int(min_y), int(max_y), 1))\n",
        "    xx, yy = np.meshgrid(range_x, range_y)\n",
        "    ba_x = xx - centerA[0]  # the vector from (x,y) to centerA\n",
        "    ba_y = yy - centerA[1]\n",
        "    limb_width = np.abs(ba_x * limb_vec_unit[1] - ba_y * limb_vec_unit[0])\n",
        "    mask = limb_width < thre  # mask is 2D\n",
        "\n",
        "    vec_map = np.copy(accumulate_vec_map) * 0.0\n",
        "    vec_map[yy, xx] = np.repeat(mask[:, :, np.newaxis], 2, axis=2)\n",
        "    vec_map[yy, xx] *= limb_vec_unit[np.newaxis, np.newaxis, :]\n",
        "\n",
        "    mask = np.logical_or.reduce(\n",
        "        (np.abs(vec_map[:, :, 0]) > 0, np.abs(vec_map[:, :, 1]) > 0))\n",
        "\n",
        "    accumulate_vec_map = np.multiply(\n",
        "        accumulate_vec_map, count[:, :, np.newaxis])\n",
        "    accumulate_vec_map += vec_map\n",
        "    count[mask == True] += 1\n",
        "\n",
        "    mask = count == 0\n",
        "\n",
        "    count[mask == True] = 1\n",
        "\n",
        "    accumulate_vec_map = np.divide(accumulate_vec_map, count[:, :, np.newaxis])\n",
        "    count[mask == True] = 0\n",
        "\n",
        "    return accumulate_vec_map, count\n",
        "\n",
        "\n",
        "def get_ground_truth(meta, mask_miss):\n",
        "    \"\"\"アノテーションとマスクデータから正しい答えを求める\"\"\"\n",
        "\n",
        "    # 初期設定\n",
        "    params_transform = dict()\n",
        "    params_transform['stride'] = 8  # 画像サイズを変更したくない場合は1にする\n",
        "    params_transform['mode'] = 5\n",
        "    params_transform['crop_size_x'] = 368\n",
        "    params_transform['crop_size_y'] = 368\n",
        "    params_transform['np'] = 56\n",
        "    params_transform['sigma'] = 7.0\n",
        "    params_transform['limb_width'] = 1.0\n",
        "\n",
        "    stride = params_transform['stride']\n",
        "    mode = params_transform['mode']\n",
        "    crop_size_y = params_transform['crop_size_y']\n",
        "    crop_size_x = params_transform['crop_size_x']\n",
        "    num_parts = params_transform['np']\n",
        "    nop = meta['numOtherPeople']\n",
        "\n",
        "    # 画像サイズ\n",
        "    grid_y = crop_size_y / stride\n",
        "    grid_x = crop_size_x / stride\n",
        "    channels = (num_parts + 1) * 2\n",
        "\n",
        "    # 格納する変数\n",
        "    heatmaps = np.zeros((int(grid_y), int(grid_x), 19))\n",
        "    pafs = np.zeros((int(grid_y), int(grid_x), 38))\n",
        "\n",
        "    mask_miss = cv2.resize(mask_miss, (0, 0), fx=1.0 / stride, fy=1.0 /\n",
        "                           stride, interpolation=cv2.INTER_CUBIC).astype(\n",
        "        np.float32)\n",
        "    mask_miss = mask_miss / 255.\n",
        "    mask_miss = np.expand_dims(mask_miss, axis=2)\n",
        "\n",
        "    # マスク変数\n",
        "    heat_mask = np.repeat(mask_miss, 19, axis=2)\n",
        "    paf_mask = np.repeat(mask_miss, 38, axis=2)\n",
        "\n",
        "    # ピンポイントの座標情報をガウス分布にぼやっとさせる\n",
        "    for i in range(18):\n",
        "        if (meta['joint_self'][i, 2] <= 1):\n",
        "            center = meta['joint_self'][i, :2]\n",
        "            gaussian_map = heatmaps[:, :, i]\n",
        "            heatmaps[:, :, i] = putGaussianMaps(\n",
        "                center, gaussian_map, params_transform)\n",
        "        for j in range(nop):\n",
        "            if (meta['joint_others'][j, i, 2] <= 1):\n",
        "                center = meta['joint_others'][j, i, :2]\n",
        "                gaussian_map = heatmaps[:, :, i]\n",
        "                heatmaps[:, :, i] = putGaussianMaps(\n",
        "                    center, gaussian_map, params_transform)\n",
        "    # pafs\n",
        "    mid_1 = [2, 9, 10, 2, 12, 13, 2, 3, 4,\n",
        "             3, 2, 6, 7, 6, 2, 1, 1, 15, 16]\n",
        "\n",
        "    mid_2 = [9, 10, 11, 12, 13, 14, 3, 4, 5,\n",
        "             17, 6, 7, 8, 18, 1, 15, 16, 17, 18]\n",
        "\n",
        "    thre = 1\n",
        "    for i in range(19):\n",
        "        # limb\n",
        "\n",
        "        count = np.zeros((int(grid_y), int(grid_x)), dtype=np.uint32)\n",
        "        if (meta['joint_self'][mid_1[i] - 1, 2] <= 1 and meta['joint_self'][mid_2[i] - 1, 2] <= 1):\n",
        "            centerA = meta['joint_self'][mid_1[i] - 1, :2]\n",
        "            centerB = meta['joint_self'][mid_2[i] - 1, :2]\n",
        "            vec_map = pafs[:, :, 2 * i:2 * i + 2]\n",
        "            #                    print vec_map.shape\n",
        "\n",
        "            pafs[:, :, 2 * i:2 * i + 2], count = putVecMaps(centerA=centerA,\n",
        "                                                            centerB=centerB,\n",
        "                                                            accumulate_vec_map=vec_map,\n",
        "                                                            count=count, params_transform=params_transform)\n",
        "        for j in range(nop):\n",
        "            if (meta['joint_others'][j, mid_1[i] - 1, 2] <= 1 and meta['joint_others'][j, mid_2[i] - 1, 2] <= 1):\n",
        "                centerA = meta['joint_others'][j, mid_1[i] - 1, :2]\n",
        "                centerB = meta['joint_others'][j, mid_2[i] - 1, :2]\n",
        "                vec_map = pafs[:, :, 2 * i:2 * i + 2]\n",
        "                pafs[:, :, 2 * i:2 * i + 2], count = putVecMaps(centerA=centerA,\n",
        "                                                                centerB=centerB,\n",
        "                                                                accumulate_vec_map=vec_map,\n",
        "                                                                count=count, params_transform=params_transform)\n",
        "    # background\n",
        "    heatmaps[:, :, -\n",
        "             1] = np.maximum(1 - np.max(heatmaps[:, :, :18], axis=2), 0.)\n",
        "\n",
        "    # Tensorに\n",
        "    heat_mask = torch.from_numpy(heat_mask)\n",
        "    heatmaps = torch.from_numpy(heatmaps)\n",
        "    paf_mask = torch.from_numpy(paf_mask)\n",
        "    pafs = torch.from_numpy(pafs)\n",
        "\n",
        "    return heat_mask, heatmaps, paf_mask, pafs\n",
        "\n",
        "\n",
        "def make_datapath_list(rootpath):\n",
        "    \"\"\"\n",
        "    学習、検証の画像データとアノテーションデータ、マスクデータへのファイルパスリストを作成する。\n",
        "    \"\"\"\n",
        "\n",
        "    # アノテーションのJSONファイルを読み込む\n",
        "    json_path = os.path.join(rootpath, 'COCO.json')\n",
        "    with open(json_path) as data_file:\n",
        "        data_this = json.load(data_file)\n",
        "        data_json = data_this['root']\n",
        "\n",
        "    # indexを格納\n",
        "    num_samples = len(data_json)\n",
        "    train_indexes = []\n",
        "    val_indexes = []\n",
        "    for count in range(num_samples):\n",
        "        if data_json[count]['isValidation'] != 0.:\n",
        "            val_indexes.append(count)\n",
        "        else:\n",
        "            train_indexes.append(count)\n",
        "\n",
        "    # 画像ファイルパスを格納\n",
        "    train_img_list = list()\n",
        "    val_img_list = list()\n",
        "\n",
        "    for idx in train_indexes:\n",
        "        img_path = os.path.join(rootpath, data_json[idx]['img_paths'])\n",
        "        train_img_list.append(img_path)\n",
        "\n",
        "    for idx in val_indexes:\n",
        "        img_path = os.path.join(rootpath, data_json[idx]['img_paths'])\n",
        "        val_img_list.append(img_path)\n",
        "\n",
        "    # マスクデータのパスを格納\n",
        "    train_mask_list = []\n",
        "    val_mask_list = []\n",
        "\n",
        "    for idx in train_indexes:\n",
        "        img_idx = data_json[idx]['img_paths'][-16:-4]\n",
        "        anno_path = \"./data/mask/train2014/mask_COCO_train2014_\" + img_idx+'.jpg'\n",
        "        train_mask_list.append(anno_path)\n",
        "\n",
        "    for idx in val_indexes:\n",
        "        img_idx = data_json[idx]['img_paths'][-16:-4]\n",
        "        anno_path = \"./data/mask/val2014/mask_COCO_val2014_\" + img_idx+'.jpg'\n",
        "        val_mask_list.append(anno_path)\n",
        "\n",
        "    # アノテーションデータを格納\n",
        "    train_meta_list = list()\n",
        "    val_meta_list = list()\n",
        "\n",
        "    for idx in train_indexes:\n",
        "        train_meta_list.append(data_json[idx])\n",
        "\n",
        "    for idx in val_indexes:\n",
        "        val_meta_list.append(data_json[idx])\n",
        "\n",
        "    return train_img_list, train_mask_list, val_img_list, val_mask_list, train_meta_list, val_meta_list\n",
        "\n",
        "\n",
        "class DataTransform():\n",
        "    \"\"\"\n",
        "    画像とマスク、アノテーションの前処理クラス。\n",
        "    学習時と推論時で異なる動作をする。\n",
        "    学習時はデータオーギュメンテーションする。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.data_transform = {\n",
        "            'train': Compose([\n",
        "                get_anno(),  # JSONからアノテーションを辞書に格納\n",
        "                add_neck(),  # アノテーションデータの順番を変更し、さらに首のアノテーションデータを追加\n",
        "                aug_scale(),  # 拡大縮小\n",
        "                aug_rotate(),  # 回転\n",
        "                aug_croppad(),  # 切り出し\n",
        "                aug_flip(),  # 左右反転\n",
        "                remove_illegal_joint(),  # 画像からはみ出たアノテーションを除去\n",
        "                Normalize_Tensor()  # 色情報の標準化とテンソル化\n",
        "                # no_Normalize_Tensor()  # 本節のみ、色情報の標準化をなくす\n",
        "            ]),\n",
        "            'val': Compose([\n",
        "                # 本書では検証は省略\n",
        "            ])\n",
        "        }\n",
        "\n",
        "    def __call__(self, phase, meta_data, img, mask_miss):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        phase : 'train' or 'val'\n",
        "            前処理のモードを指定。\n",
        "        \"\"\"\n",
        "        meta_data, img, mask_miss = self.data_transform[phase](\n",
        "            meta_data, img, mask_miss)\n",
        "\n",
        "        return meta_data, img, mask_miss\n",
        "\n",
        "\n",
        "class COCOkeypointsDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    MSCOCOのCocokeypointsのDatasetを作成するクラス。PyTorchのDatasetクラスを継承。\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    img_list : リスト\n",
        "        画像のパスを格納したリスト\n",
        "    anno_list : リスト\n",
        "        アノテーションへのパスを格納したリスト\n",
        "    phase : 'train' or 'test'\n",
        "        学習か訓練かを設定する。\n",
        "    transform : object\n",
        "        前処理クラスのインスタンス\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_list, mask_list, meta_list, phase, transform):\n",
        "        self.img_list = img_list\n",
        "        self.mask_list = mask_list\n",
        "        self.meta_list = meta_list\n",
        "        self.phase = phase\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        '''画像の枚数を返す'''\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, heatmaps, heat_mask, pafs, paf_mask = self.pull_item(index)\n",
        "        return img, heatmaps, heat_mask, pafs, paf_mask\n",
        "\n",
        "    def pull_item(self, index):\n",
        "        '''画像のTensor形式のデータ、アノテーション、マスクを取得する'''\n",
        "\n",
        "        # 1. 画像読み込み\n",
        "        image_file_path = self.img_list[index]\n",
        "        img = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\n",
        "\n",
        "        # 2. マスクとアノテーション読み込み\n",
        "        mask_miss = cv2.imread(self.mask_list[index])\n",
        "        meat_data = self.meta_list[index]\n",
        "\n",
        "        # 3. 画像前処理\n",
        "        meta_data, img, mask_miss = self.transform(\n",
        "            self.phase, meat_data, img, mask_miss)\n",
        "\n",
        "        # 4. 正解アノテーションデータの取得\n",
        "        mask_miss_numpy = mask_miss.numpy().transpose((1, 2, 0))\n",
        "        heat_mask, heatmaps, paf_mask, pafs = get_ground_truth(\n",
        "            meta_data, mask_miss_numpy)\n",
        "\n",
        "        # 5. マスクデータはRGBが(1,1,1)か(0,0,0)なので、次元を落とす\n",
        "        heat_mask = heat_mask[:, :, :, 0]\n",
        "        paf_mask = paf_mask[:, :, :, 0]\n",
        "\n",
        "        # 6. チャネルが最後尾にあるので順番を変える\n",
        "        # 例：paf_mask：torch.Size([46, 46, 38])\n",
        "        # → torch.Size([38, 46, 46])\n",
        "        paf_mask = paf_mask.permute(2, 0, 1)\n",
        "        heat_mask = heat_mask.permute(2, 0, 1)\n",
        "        pafs = pafs.permute(2, 0, 1)\n",
        "        heatmaps = heatmaps.permute(2, 0, 1)\n",
        "\n",
        "        return img, heatmaps, heat_mask, pafs, paf_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOu6aNghQIQ4"
      },
      "source": [
        "# 画像読み込み\n",
        "index = 24\n",
        "img = cv2.imread(val_img_list[index])\n",
        "mask_miss = cv2.imread(val_mask_list[index])\n",
        "meat_data = val_meta_list[index]\n",
        "\n",
        "# 画像前処理\n",
        "meta_data, img, mask_miss = transform(\"train\", meat_data, img, mask_miss)\n",
        "\n",
        "img = img.numpy().transpose((1, 2, 0))\n",
        "mask_miss = mask_miss.numpy().transpose((1, 2, 0))\n",
        "\n",
        "# OpenPoseのアノテーションデータ生成\n",
        "heat_mask, heatmaps, paf_mask, pafs = get_ground_truth(meta_data, mask_miss)\n",
        "\n",
        "# 画像表示\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh95SdjYQOY6"
      },
      "source": [
        "class COCOkeypointsDataset(data.Dataset):\n",
        "    def __init__(self, img_list, mask_list, meta_list, phase, transform):\n",
        "        self.img_list = img_list\n",
        "        self.mask_list = mask_list\n",
        "        self.meta_list = meta_list\n",
        "        self.phase = phase\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        '''画像の枚数を返す'''\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, heatmaps, heat_mask, pafs, paf_mask = self.pull_item(index)\n",
        "        return img, heatmaps, heat_mask, pafs, paf_mask\n",
        "\n",
        "    def pull_item(self, index):\n",
        "        '''画像のTensor形式のデータ、アノテーション、マスクを取得する'''\n",
        "\n",
        "        # 1. 画像読み込み\n",
        "        image_file_path = self.img_list[index]\n",
        "        img = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\n",
        "\n",
        "        # 2. マスクとアノテーション読み込み\n",
        "        mask_miss = cv2.imread(self.mask_list[index])\n",
        "        meat_data = self.meta_list[index]\n",
        "\n",
        "        # 3. 画像前処理\n",
        "        meta_data, img, mask_miss = self.transform(\n",
        "            self.phase, meat_data, img, mask_miss)\n",
        "\n",
        "        # 4. 正解アノテーションデータの取得\n",
        "        mask_miss_numpy = mask_miss.numpy().transpose((1, 2, 0))\n",
        "        heat_mask, heatmaps, paf_mask, pafs = get_ground_truth(\n",
        "            meta_data, mask_miss_numpy)\n",
        "\n",
        "        # 5. マスクデータはRGBが(1,1,1)か(0,0,0)なので、次元を落とす\n",
        "        # マスクデータはマスクされている場所は値が0、それ以外は値が1です\n",
        "        heat_mask = heat_mask[:, :, :, 0]\n",
        "        paf_mask = paf_mask[:, :, :, 0]\n",
        "\n",
        "        # 6. チャネルが最後尾にあるので順番を変える\n",
        "        # 例：paf_mask：torch.Size([46, 46, 38])\n",
        "        # → torch.Size([38, 46, 46])\n",
        "        paf_mask = paf_mask.permute(2, 0, 1)\n",
        "        heat_mask = heat_mask.permute(2, 0, 1)\n",
        "        pafs = pafs.permute(2, 0, 1)\n",
        "        heatmaps = heatmaps.permute(2, 0, 1)\n",
        "\n",
        "        return img, heatmaps, heat_mask, pafs, paf_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKC-SkfhQgX8"
      },
      "source": [
        "# 動作確認\n",
        "train_dataset = COCOkeypointsDataset(\n",
        "    val_img_list, val_mask_list, val_meta_list, phase=\"train\", transform=DataTransform())\n",
        "val_dataset = COCOkeypointsDataset(\n",
        "    val_img_list, val_mask_list, val_meta_list, phase=\"val\", transform=DataTransform())\n",
        "\n",
        "\n",
        "# データの取り出し例\n",
        "item = train_dataset.__getitem__(0)\n",
        "print(item[0].shape)  # img\n",
        "print(item[1].shape)  # heatmaps,\n",
        "print(item[2].shape)  # heat_mask\n",
        "print(item[3].shape)  # pafs \n",
        "print(item[4].shape)  # paf_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-74oaVb8QkN6"
      },
      "source": [
        "DataLoaderの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce6y3yiuQo0B"
      },
      "source": [
        "batch_size = 8\n",
        "\n",
        "train_dataloader = data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataloader = data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 辞書型変数にまとめる\n",
        "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
        "\n",
        "# 動作の確認\n",
        "batch_iterator = iter(dataloaders_dict[\"train\"])  # イタレータに変換\n",
        "item = next(batch_iterator)  # 1番目の要素を取り出す\n",
        "print(item[0].shape)  # img\n",
        "print(item[1].shape)  # heatmaps,\n",
        "print(item[2].shape)  # heat_mask\n",
        "print(item[3].shape)  # pafs \n",
        "print(item[4].shape)  # paf_mask"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}