{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Openpose.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNLur1b6h7Y0PcqeBDhZH9f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomohitom/Tennis_Game_Analysis/blob/master/Openpose.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_l0MJA_sRmU",
        "outputId": "0db32e47-cd88-4c71-cce5-d2ead2bdcd23"
      },
      "source": [
        "!git clone \"https://github.com/YutaroOgawa/pytorch_advanced.git\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch_advanced'...\n",
            "remote: Enumerating objects: 529, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 529 (delta 23), reused 32 (delta 13), pack-reused 479\u001b[K\n",
            "Receiving objects: 100% (529/529), 17.57 MiB | 22.27 MiB/s, done.\n",
            "Resolving deltas: 100% (282/282), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig9q8g2lsma-"
      },
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tarfile\n",
        "\n",
        "data_dir = \"/content/pytorch_advanced/4_pose_estimation/data/\"\n",
        "if not os.path.exists(data_dir):\n",
        "    os.mkdir(data_dir)\n",
        "\n",
        "weights_dir = \"/content/pytorch_advanced/4_pose_estimation/weights/\"\n",
        "if not os.path.exists(weights_dir):\n",
        "    os.mkdir(weights_dir)\n",
        "\n",
        "url =  \"http://images.cocodataset.org/zips/val2014.zip\"\n",
        "target_path = os.path.join(data_dir, \"val2014.zip\") \n",
        "\n",
        "if not os.path.exists(target_path):\n",
        "    urllib.request.urlretrieve(url, target_path)\n",
        "    \n",
        "    zip = zipfile.ZipFile(target_path)\n",
        "    zip.extractall(data_dir)  # ZIPを解凍\n",
        "    zip.close()  # ZIPファイルをクローズ"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1APQzpO3OOa"
      },
      "source": [
        "COCO.jsonとmask.tar.gzを「data」ディレクトリにアップロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeDeGx8d3DwK"
      },
      "source": [
        "save_path = os.path.join(\"/content/pytorch_advanced/4_pose_estimation/data\", \"mask.tar.gz\") \n",
        "\n",
        "with tarfile.open(save_path, 'r:*') as tar:\n",
        "    tar.extractall(\"/content/pytorch_advanced/4_pose_estimation/data\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geriDtKrKQ6C"
      },
      "source": [
        "画像の前処理作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQwPJE25KUxg"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, meta_data, img, mask_miss):\n",
        "        for t in self.transforms:\n",
        "            meta_data, img, mask_miss = t(meta_data, img, mask_miss)\n",
        "\n",
        "        return meta_data, img, mask_miss\n",
        "\n",
        "\n",
        "class get_anno(object):\n",
        "    def __call__(self, meta_data, img, mask_miss):\n",
        "        anno = dict()\n",
        "        anno['dataset'] = meta_data['dataset']\n",
        "        anno['img_height'] = int(meta_data['img_height'])\n",
        "        anno['img_width'] = int(meta_data['img_width'])\n",
        "\n",
        "        anno['isValidation'] = meta_data['isValidation']\n",
        "        anno['people_index'] = int(meta_data['people_index'])\n",
        "        anno['annolist_index'] = int(meta_data['annolist_index'])\n",
        "\n",
        "        # (b) objpos_x (float), objpos_y (float)\n",
        "        anno['objpos'] = np.array(meta_data['objpos'])\n",
        "        anno['scale_provided'] = meta_data['scale_provided']\n",
        "        anno['joint_self'] = np.array(meta_data['joint_self'])\n",
        "\n",
        "        anno['numOtherPeople'] = int(meta_data['numOtherPeople'])\n",
        "        anno['num_keypoints_other'] = np.array(\n",
        "            meta_data['num_keypoints_other'])\n",
        "        anno['joint_others'] = np.array(meta_data['joint_others'])\n",
        "        anno['objpos_other'] = np.array(meta_data['objpos_other'])\n",
        "        anno['scale_provided_other'] = meta_data['scale_provided_other']\n",
        "        anno['bbox_other'] = meta_data['bbox_other']\n",
        "        anno['segment_area_other'] = meta_data['segment_area_other']\n",
        "\n",
        "        if anno['numOtherPeople'] == 1:\n",
        "            anno['joint_others'] = np.expand_dims(anno['joint_others'], 0)\n",
        "            anno['objpos_other'] = np.expand_dims(anno['objpos_other'], 0)\n",
        "\n",
        "        meta_data = anno\n",
        "\n",
        "        return meta_data, img, mask_miss\n",
        "\n",
        "\n",
        "class add_neck(object):\n",
        "    '''\n",
        "    アノテーションデータの順番を変更し、さらに首のアノテーションデータを追加します。\n",
        "    首ポジションは両肩の位置から計算します。\n",
        "\n",
        "    MS COCO annotation order:\n",
        "    0: nose\t   \t\t1: l eye\t\t2: r eye\t3: l ear\t4: r ear\n",
        "    5: l shoulder\t6: r shoulder\t7: l elbow\t8: r elbow\n",
        "    9: l wrist\t\t10: r wrist\t\t11: l hip\t12: r hip\t13: l knee\n",
        "    14: r knee\t\t15: l ankle\t\t16: r ankle\n",
        "    The order in this work:\n",
        "    (0-'nose'\t1-'neck' 2-'right_shoulder' 3-'right_elbow' 4-'right_wrist'\n",
        "    5-'left_shoulder' 6-'left_elbow'\t    7-'left_wrist'  8-'right_hip'\n",
        "    9-'right_knee'\t 10-'right_ankle'\t11-'left_hip'   12-'left_knee'\n",
        "    13-'left_ankle'\t 14-'right_eye'\t    15-'left_eye'   16-'right_ear'\n",
        "    17-'left_ear' )\n",
        "    '''\n",
        "\n",
        "    def __call__(self, meta_data, img, mask_miss):\n",
        "        meta = meta_data\n",
        "        our_order = [0, 17, 6, 8, 10, 5, 7, 9,\n",
        "                     12, 14, 16, 11, 13, 15, 2, 1, 4, 3]\n",
        "        # Index 6 is right shoulder and Index 5 is left shoulder\n",
        "        right_shoulder = meta['joint_self'][6, :]\n",
        "        left_shoulder = meta['joint_self'][5, :]\n",
        "        neck = (right_shoulder + left_shoulder) / 2\n",
        "\n",
        "        # right_shoulder[2]が値1のときはアノテーションがあり画像内に部位も見えている\n",
        "        # 値0のときはアノテーションの座標情報はあるが、画像内に部位は映っていない\n",
        "        # 値が2のときは画像内に写っておらず、アノテーション付けもない\n",
        "        # ※注意　元のMSCOCOの定義と値の意味が変わっている\n",
        "        # v=0: not labeled (in which case x=y=0), v=1: labeled but not visible, and v=2: labeled and visible.\n",
        "        if right_shoulder[2] == 2 or left_shoulder[2] == 2:\n",
        "            neck[2] = 2\n",
        "        elif right_shoulder[2] == 1 or left_shoulder[2] == 1:\n",
        "            neck[2] = 1\n",
        "        else:\n",
        "            neck[2] = right_shoulder[2] * left_shoulder[2]\n",
        "\n",
        "        neck = neck.reshape(1, len(neck))\n",
        "        neck = np.round(neck)\n",
        "        meta['joint_self'] = np.vstack((meta['joint_self'], neck))\n",
        "        meta['joint_self'] = meta['joint_self'][our_order, :]\n",
        "        temp = []\n",
        "\n",
        "        for i in range(meta['numOtherPeople']):\n",
        "            right_shoulder = meta['joint_others'][i, 6, :]\n",
        "            left_shoulder = meta['joint_others'][i, 5, :]\n",
        "            neck = (right_shoulder + left_shoulder) / 2\n",
        "            if (right_shoulder[2] == 2 or left_shoulder[2] == 2):\n",
        "                neck[2] = 2\n",
        "            elif (right_shoulder[2] == 1 or left_shoulder[2] == 1):\n",
        "                neck[2] = 1\n",
        "            else:\n",
        "                neck[2] = right_shoulder[2] * left_shoulder[2]\n",
        "            neck = neck.reshape(1, len(neck))\n",
        "            neck = np.round(neck)\n",
        "            single_p = np.vstack((meta['joint_others'][i], neck))\n",
        "            single_p = single_p[our_order, :]\n",
        "            temp.append(single_p)\n",
        "        meta['joint_others'] = np.array(temp)\n",
        "\n",
        "        meta_data = meta\n",
        "\n",
        "        return meta_data, img, mask_miss\n",
        "\n",
        "\n",
        "class aug_scale(object):\n",
        "    def __init__(self):\n",
        "        self.params_transform = dict()\n",
        "        self.params_transform['scale_min'] = 0.5\n",
        "        self.params_transform['scale_max'] = 1.1\n",
        "        self.params_transform['target_dist'] = 0.6\n",
        "\n",
        "    def __call__(self, meta_data, img, mask_miss):\n",
        "\n",
        "        # ランダムに0.5倍～1.1倍する\n",
        "        dice = random.random()  # (0,1)\n",
        "        scale_multiplier = (\n",
        "            self.params_transform['scale_max'] - self.params_transform['scale_min']) * dice + self.params_transform['scale_min']\n",
        "\n",
        "        scale_abs = self.params_transform['target_dist'] / \\\n",
        "            meta_data['scale_provided']\n",
        "        scale = scale_abs * scale_multiplier\n",
        "        img = cv2.resize(img, (0, 0), fx=scale, fy=scale,\n",
        "                         interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "        mask_miss = cv2.resize(mask_miss, (0, 0), fx=scale,\n",
        "                               fy=scale, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "        # modify meta data\n",
        "        meta_data['objpos'] *= scale\n",
        "        meta_data['joint_self'][:, :2] *= scale\n",
        "        if (meta_data['numOtherPeople'] != 0):\n",
        "            meta_data['objpos_other'] *= scale\n",
        "            meta_data['joint_others'][:, :, :2] *= scale\n",
        "        return meta_data, img, mask_miss\n",
        "\n",
        "\n",
        "class aug_rotate(object):\n",
        "    def __init__(self):\n",
        "        self.params_transform = dict()\n",
        "        self.params_transform['max_rotate_degree'] = 40\n",
        "\n",
        "    def __call__(self, meta_data, img, mask_miss):\n",
        "\n",
        "        # ランダムに-40～40度回転\n",
        "        dice = random.random()  # (0,1)\n",
        "        degree = (dice - 0.5) * 2 * \\\n",
        "            self.params_transform['max_rotate_degree']  # degree [-40,40]\n",
        "\n",
        "        def rotate_bound(image, angle, bordervalue):\n",
        "            # grab the dimensions of the image and then determine the\n",
        "            # center\n",
        "            (h, w) = image.shape[:2]\n",
        "            (cX, cY) = (w // 2, h // 2)\n",
        "\n",
        "            # grab the rotation matrix (applying the negative of the\n",
        "            # angle to rotate clockwise), then grab the sine and cosine\n",
        "            # (i.e., the rotation components of the matrix)\n",
        "            M = cv2.getRotationMatrix2D((cX, cY), -angle, 1.0)\n",
        "            cos = np.abs(M[0, 0])\n",
        "            sin = np.abs(M[0, 1])\n",
        "\n",
        "            # compute the new bounding dimensions of the image\n",
        "            nW = int((h * sin) + (w * cos))\n",
        "            nH = int((h * cos) + (w * sin))\n",
        "\n",
        "            # adjust the rotation matrix to take into account translation\n",
        "            M[0, 2] += (nW / 2) - cX\n",
        "            M[1, 2] += (nH / 2) - cY\n",
        "\n",
        "            # perform the actual rotation and return the image\n",
        "            return cv2.warpAffine(image, M, (nW, nH), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_CONSTANT,\n",
        "                                  borderValue=bordervalue), M\n",
        "\n",
        "        def rotatepoint(p, R):\n",
        "            point = np.zeros((3, 1))\n",
        "            point[0] = p[0]\n",
        "            point[1] = p[1]\n",
        "            point[2] = 1\n",
        "\n",
        "            new_point = R.dot(point)\n",
        "\n",
        "            p[0] = new_point[0]\n",
        "\n",
        "            p[1] = new_point[1]\n",
        "            return p\n",
        "\n",
        "        # 画像とマスク画像の回転\n",
        "        img_rot, R = rotate_bound(img, np.copy(\n",
        "            degree), (128, 128, 128))  # 回転でできた隙間は青色\n",
        "        mask_miss_rot, _ = rotate_bound(\n",
        "            mask_miss, np.copy(degree), (255, 255, 255))\n",
        "\n",
        "        # アノテーションデータの回転\n",
        "        meta_data['objpos'] = rotatepoint(meta_data['objpos'], R)\n",
        "\n",
        "        for i in range(18):\n",
        "            meta_data['joint_self'][i, :] = rotatepoint(\n",
        "                meta_data['joint_self'][i, :], R)\n",
        "\n",
        "        for j in range(meta_data['numOtherPeople']):\n",
        "\n",
        "            meta_data['objpos_other'][j, :] = rotatepoint(\n",
        "                meta_data['objpos_other'][j, :], R)\n",
        "\n",
        "            for i in range(18):\n",
        "                meta_data['joint_others'][j, i, :] = rotatepoint(\n",
        "                    meta_data['joint_others'][j, i, :], R)\n",
        "\n",
        "        return meta_data, img_rot, mask_miss_rot\n",
        "\n",
        "\n",
        "class aug_croppad(object):\n",
        "    def __init__(self):\n",
        "        self.params_transform = dict()\n",
        "        self.params_transform['center_perterb_max'] = 40\n",
        "        self.params_transform['crop_size_x'] = 368\n",
        "        self.params_transform['crop_size_y'] = 368\n",
        "\n",
        "    def __call__(self, meta_data, img, mask_miss):\n",
        "\n",
        "        # ランダムにオフセットを用意 -40から40\n",
        "        dice_x = random.random()  # (0,1)\n",
        "        dice_y = random.random()  # (0,1)\n",
        "        crop_x = int(self.params_transform['crop_size_x'])\n",
        "        crop_y = int(self.params_transform['crop_size_y'])\n",
        "        x_offset = int((dice_x - 0.5) * 2 *\n",
        "                       self.params_transform['center_perterb_max'])\n",
        "        y_offset = int((dice_y - 0.5) * 2 *\n",
        "                       self.params_transform['center_perterb_max'])\n",
        "\n",
        "        center = meta_data['objpos'] + np.array([x_offset, y_offset])\n",
        "        center = center.astype(int)\n",
        "\n",
        "        # pad up and down\n",
        "        # img.shape=（幅、高さ）\n",
        "        pad_v = np.ones((crop_y, img.shape[1], 3), dtype=np.uint8) * 128\n",
        "        pad_v_mask_miss = np.ones(\n",
        "            (crop_y, mask_miss.shape[1], 3), dtype=np.uint8) * 255\n",
        "        img = np.concatenate((pad_v, img, pad_v), axis=0)\n",
        "\n",
        "        mask_miss = np.concatenate(\n",
        "            (pad_v_mask_miss, mask_miss, pad_v_mask_miss), axis=0)\n",
        "\n",
        "        # pad right and left\n",
        "        # img.shape=（幅、高さ）\n",
        "        pad_h = np.ones((img.shape[0], crop_x, 3), dtype=np.uint8) * 128\n",
        "        pad_h_mask_miss = np.ones(\n",
        "            (mask_miss.shape[0], crop_x, 3), dtype=np.uint8) * 255\n",
        "\n",
        "        img = np.concatenate((pad_h, img, pad_h), axis=1)\n",
        "        mask_miss = np.concatenate(\n",
        "            (pad_h_mask_miss, mask_miss, pad_h_mask_miss), axis=1)\n",
        "\n",
        "        # 切り出す\n",
        "        img = img[int(center[1] + crop_y / 2):int(center[1] + crop_y / 2 + crop_y),\n",
        "                  int(center[0] + crop_x / 2):int(center[0] + crop_x / 2 + crop_x), :]\n",
        "\n",
        "        mask_miss = mask_miss[int(center[1] + crop_y / 2):int(center[1] + crop_y / 2 +\n",
        "                                                              crop_y + 1*0), int(center[0] + crop_x / 2):int(center[0] + crop_x / 2 + crop_x + 1*0)]\n",
        "\n",
        "        offset_left = crop_x / 2 - center[0]\n",
        "        offset_up = crop_y / 2 - center[1]\n",
        "\n",
        "        offset = np.array([offset_left, offset_up])\n",
        "        meta_data['objpos'] += offset\n",
        "        meta_data['joint_self'][:, :2] += offset\n",
        "\n",
        "        # 画像からはみ出ていないかチェック\n",
        "        # 条件式4つのORを計算する\n",
        "        mask = np.logical_or.reduce((meta_data['joint_self'][:, 0] >= crop_x,\n",
        "                                     meta_data['joint_self'][:, 0] < 0,\n",
        "                                     meta_data['joint_self'][:, 1] >= crop_y,\n",
        "                                     meta_data['joint_self'][:, 1] < 0))\n",
        "\n",
        "        meta_data['joint_self'][mask == True, 2] = 2\n",
        "        if (meta_data['numOtherPeople'] != 0):\n",
        "            meta_data['objpos_other'] += offset\n",
        "            meta_data['joint_others'][:, :, :2] += offset\n",
        "\n",
        "            # 条件式4つのORを計算する\n",
        "            mask = np.logical_or.reduce((meta_data['joint_others'][:, :, 0] >= crop_x,\n",
        "                                         meta_data['joint_others'][:,\n",
        "                                                                   :, 0] < 0,\n",
        "                                         meta_data['joint_others'][:,\n",
        "                                                                   :, 1] >= crop_y,\n",
        "                                         meta_data['joint_others'][:, :, 1] < 0))\n",
        "\n",
        "            meta_data['joint_others'][mask == True, 2] = 2\n",
        "\n",
        "        return meta_data, img, mask_miss\n",
        "\n",
        "\n",
        "class aug_flip(object):\n",
        "    def __init__(self):\n",
        "        self.params_transform = dict()\n",
        "        self.params_transform['flip_prob'] = 0.5\n",
        "\n",
        "    def __call__(self, meta_data, img, mask_miss):\n",
        "\n",
        "        # ランダムにオフセットを用意 -40から40\n",
        "\n",
        "        dice = random.random()  # (0,1)\n",
        "        doflip = dice <= self.params_transform['flip_prob']\n",
        "\n",
        "        if doflip:\n",
        "            img = img.copy()\n",
        "            cv2.flip(src=img, flipCode=1, dst=img)\n",
        "            w = img.shape[1]  # img.shape=（幅、高さ）\n",
        "\n",
        "            mask_miss = mask_miss.copy()\n",
        "            cv2.flip(src=mask_miss, flipCode=1, dst=mask_miss)\n",
        "\n",
        "            '''\n",
        "            The order in this work:\n",
        "                (0-'nose'   1-'neck' 2-'right_shoulder' 3-'right_elbow' 4-'right_wrist'\n",
        "                5-'left_shoulder' 6-'left_elbow'        7-'left_wrist'  8-'right_hip'  \n",
        "                9-'right_knee'   10-'right_ankle'   11-'left_hip'   12-'left_knee' \n",
        "                13-'left_ankle'  14-'right_eye'     15-'left_eye'   16-'right_ear' \n",
        "                17-'left_ear' )\n",
        "            '''\n",
        "            meta_data['objpos'][0] = w - 1 - meta_data['objpos'][0]\n",
        "            meta_data['joint_self'][:, 0] = w - \\\n",
        "                1 - meta_data['joint_self'][:, 0]\n",
        "            # print meta['joint_self']\n",
        "            meta_data['joint_self'] = meta_data['joint_self'][[0, 1, 5, 6,\n",
        "                                                               7, 2, 3, 4, 11, 12, 13, 8, 9, 10, 15, 14, 17, 16]]\n",
        "\n",
        "            num_other_people = meta_data['numOtherPeople']\n",
        "            if (num_other_people != 0):\n",
        "                meta_data['objpos_other'][:, 0] = w - \\\n",
        "                    1 - meta_data['objpos_other'][:, 0]\n",
        "                meta_data['joint_others'][:, :, 0] = w - \\\n",
        "                    1 - meta_data['joint_others'][:, :, 0]\n",
        "                for i in range(num_other_people):\n",
        "                    meta_data['joint_others'][i] = meta_data['joint_others'][i][[\n",
        "                        0, 1, 5, 6, 7, 2, 3, 4, 11, 12, 13, 8, 9, 10, 15, 14, 17, 16]]\n",
        "\n",
        "        return meta_data, img, mask_miss\n",
        "\n",
        "\n",
        "class remove_illegal_joint(object):\n",
        "    \"\"\"データオーギュメンテーションの結果、画像内から外れたパーツの位置情報を変更する\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.params_transform = dict()\n",
        "        self.params_transform['crop_size_x'] = 368\n",
        "        self.params_transform['crop_size_y'] = 368\n",
        "\n",
        "    def __call__(self, meta_data, img, mask_miss):\n",
        "        crop_x = int(self.params_transform['crop_size_x'])\n",
        "        crop_y = int(self.params_transform['crop_size_y'])\n",
        "\n",
        "        # 条件式4つのORを計算する\n",
        "        mask = np.logical_or.reduce((meta_data['joint_self'][:, 0] >= crop_x,\n",
        "                                     meta_data['joint_self'][:, 0] < 0,\n",
        "                                     meta_data['joint_self'][:, 1] >= crop_y,\n",
        "                                     meta_data['joint_self'][:, 1] < 0))\n",
        "\n",
        "        # 画像内の枠からはみ出たパーツの位置情報は(1,1,2)にする\n",
        "        meta_data['joint_self'][mask == True, :] = (1, 1, 2)\n",
        "\n",
        "        if (meta_data['numOtherPeople'] != 0):\n",
        "            mask = np.logical_or.reduce((meta_data['joint_others'][:, :, 0] >= crop_x,\n",
        "                                         meta_data['joint_others'][:,\n",
        "                                                                   :, 0] < 0,\n",
        "                                         meta_data['joint_others'][:,\n",
        "                                                                   :, 1] >= crop_y,\n",
        "                                         meta_data['joint_others'][:, :, 1] < 0))\n",
        "            meta_data['joint_others'][mask == True, :] = (1, 1, 2)\n",
        "\n",
        "        return meta_data, img, mask_miss\n",
        "\n",
        "\n",
        "class Normalize_Tensor(object):\n",
        "    def __init__(self):\n",
        "        self.color_mean = [0.485, 0.456, 0.406]\n",
        "        self.color_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    def __call__(self, meta_data, img, mask_miss):\n",
        "\n",
        "        # 画像の大きさは最大1に規格化される\n",
        "        img = img.astype(np.float32) / 255.\n",
        "\n",
        "        # 色情報の標準化\n",
        "        preprocessed_img = img.copy()[:, :, ::-1]  # BGR→RGB\n",
        "\n",
        "        for i in range(3):\n",
        "            preprocessed_img[:, :, i] = preprocessed_img[:,\n",
        "                                                         :, i] - self.color_mean[i]\n",
        "            preprocessed_img[:, :, i] = preprocessed_img[:,\n",
        "                                                         :, i] / self.color_std[i]\n",
        "\n",
        "        # （幅、高さ、色）→（色、幅、高さ）\n",
        "        img = preprocessed_img.transpose((2, 0, 1)).astype(np.float32)\n",
        "        mask_miss = mask_miss.transpose((2, 0, 1)).astype(np.float32)\n",
        "\n",
        "        # 画像をTensorに\n",
        "        img = torch.from_numpy(img)\n",
        "        mask_miss = torch.from_numpy(mask_miss)\n",
        "\n",
        "        return meta_data, img, mask_miss\n",
        "\n",
        "\n",
        "class no_Normalize_Tensor(object):\n",
        "    def __init__(self):\n",
        "        self.color_mean = [0, 0, 0]\n",
        "        self.color_std = [1, 1, 1]\n",
        "\n",
        "    def __call__(self, meta_data, img, mask_miss):\n",
        "\n",
        "        # 画像の大きさは最大1に規格化される\n",
        "        img = img.astype(np.float32) / 255.\n",
        "\n",
        "        # 色情報の標準化\n",
        "        preprocessed_img = img.copy()[:, :, ::-1]  # BGR→RGB\n",
        "\n",
        "        for i in range(3):\n",
        "            preprocessed_img[:, :, i] = preprocessed_img[:,\n",
        "                                                         :, i] - self.color_mean[i]\n",
        "            preprocessed_img[:, :, i] = preprocessed_img[:,\n",
        "                                                         :, i] / self.color_std[i]\n",
        "\n",
        "        # （幅、高さ、色）→（色、幅、高さ）\n",
        "        img = preprocessed_img.transpose((2, 0, 1)).astype(np.float32)\n",
        "        mask_miss = mask_miss.transpose((2, 0, 1)).astype(np.float32)\n",
        "\n",
        "        # 画像をTensorに\n",
        "        img = torch.from_numpy(img)\n",
        "        mask_miss = torch.from_numpy(mask_miss)\n",
        "\n",
        "        return meta_data, img, mask_miss"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k4tMLhZPFxV"
      },
      "source": [
        "\n",
        "訓練データの正解情報として使うアノテーションデータの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txjz7VYBO_5u"
      },
      "source": [
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from scipy import misc, ndimage\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "\n",
        "def putGaussianMaps(center, accumulate_confid_map, params_transform):\n",
        "    crop_size_y = params_transform['crop_size_y']\n",
        "    crop_size_x = params_transform['crop_size_x']\n",
        "    stride = params_transform['stride']\n",
        "    sigma = params_transform['sigma']\n",
        "\n",
        "    grid_y = crop_size_y / stride\n",
        "    grid_x = crop_size_x / stride\n",
        "    start = stride / 2.0 - 0.5\n",
        "    y_range = [i for i in range(int(grid_y))]\n",
        "    x_range = [i for i in range(int(grid_x))]\n",
        "    xx, yy = np.meshgrid(x_range, y_range)\n",
        "    xx = xx * stride + start\n",
        "    yy = yy * stride + start\n",
        "    d2 = (xx - center[0]) ** 2 + (yy - center[1]) ** 2\n",
        "    exponent = d2 / 2.0 / sigma / sigma\n",
        "    mask = exponent <= 4.6052\n",
        "    cofid_map = np.exp(-exponent)\n",
        "    cofid_map = np.multiply(mask, cofid_map)\n",
        "    accumulate_confid_map += cofid_map\n",
        "    accumulate_confid_map[accumulate_confid_map > 1.0] = 1.0\n",
        "\n",
        "    return accumulate_confid_map\n",
        "\n",
        "\n",
        "def putVecMaps(centerA, centerB, accumulate_vec_map, count, params_transform):\n",
        "    '''Parts A Fieldのベクトルを求める'''\n",
        "\n",
        "    centerA = centerA.astype(float)\n",
        "    centerB = centerB.astype(float)\n",
        "\n",
        "    stride = params_transform['stride']\n",
        "    crop_size_y = params_transform['crop_size_y']\n",
        "    crop_size_x = params_transform['crop_size_x']\n",
        "    grid_y = crop_size_y / stride\n",
        "    grid_x = crop_size_x / stride\n",
        "    thre = params_transform['limb_width']   # limb width\n",
        "    centerB = centerB / stride\n",
        "    centerA = centerA / stride\n",
        "\n",
        "    limb_vec = centerB - centerA\n",
        "    norm = np.linalg.norm(limb_vec)\n",
        "    if (norm == 0.0):\n",
        "        # print 'limb is too short, ignore it...'\n",
        "        return accumulate_vec_map, count\n",
        "    limb_vec_unit = limb_vec / norm\n",
        "    # print 'limb unit vector: {}'.format(limb_vec_unit)\n",
        "\n",
        "    # To make sure not beyond the border of this two points\n",
        "    min_x = max(int(round(min(centerA[0], centerB[0]) - thre)), 0)\n",
        "    max_x = min(int(round(max(centerA[0], centerB[0]) + thre)), grid_x)\n",
        "    min_y = max(int(round(min(centerA[1], centerB[1]) - thre)), 0)\n",
        "    max_y = min(int(round(max(centerA[1], centerB[1]) + thre)), grid_y)\n",
        "\n",
        "    range_x = list(range(int(min_x), int(max_x), 1))\n",
        "    range_y = list(range(int(min_y), int(max_y), 1))\n",
        "    xx, yy = np.meshgrid(range_x, range_y)\n",
        "    ba_x = xx - centerA[0]  # the vector from (x,y) to centerA\n",
        "    ba_y = yy - centerA[1]\n",
        "    limb_width = np.abs(ba_x * limb_vec_unit[1] - ba_y * limb_vec_unit[0])\n",
        "    mask = limb_width < thre  # mask is 2D\n",
        "\n",
        "    vec_map = np.copy(accumulate_vec_map) * 0.0\n",
        "    vec_map[yy, xx] = np.repeat(mask[:, :, np.newaxis], 2, axis=2)\n",
        "    vec_map[yy, xx] *= limb_vec_unit[np.newaxis, np.newaxis, :]\n",
        "\n",
        "    mask = np.logical_or.reduce(\n",
        "        (np.abs(vec_map[:, :, 0]) > 0, np.abs(vec_map[:, :, 1]) > 0))\n",
        "\n",
        "    accumulate_vec_map = np.multiply(\n",
        "        accumulate_vec_map, count[:, :, np.newaxis])\n",
        "    accumulate_vec_map += vec_map\n",
        "    count[mask == True] += 1\n",
        "\n",
        "    mask = count == 0\n",
        "\n",
        "    count[mask == True] = 1\n",
        "\n",
        "    accumulate_vec_map = np.divide(accumulate_vec_map, count[:, :, np.newaxis])\n",
        "    count[mask == True] = 0\n",
        "\n",
        "    return accumulate_vec_map, count\n",
        "\n",
        "\n",
        "def get_ground_truth(meta, mask_miss):\n",
        "    \"\"\"アノテーションとマスクデータから正しい答えを求める\"\"\"\n",
        "\n",
        "    # 初期設定\n",
        "    params_transform = dict()\n",
        "    params_transform['stride'] = 8  # 画像サイズを変更したくない場合は1にする\n",
        "    params_transform['mode'] = 5\n",
        "    params_transform['crop_size_x'] = 368\n",
        "    params_transform['crop_size_y'] = 368\n",
        "    params_transform['np'] = 56\n",
        "    params_transform['sigma'] = 7.0\n",
        "    params_transform['limb_width'] = 1.0\n",
        "\n",
        "    stride = params_transform['stride']\n",
        "    mode = params_transform['mode']\n",
        "    crop_size_y = params_transform['crop_size_y']\n",
        "    crop_size_x = params_transform['crop_size_x']\n",
        "    num_parts = params_transform['np']\n",
        "    nop = meta['numOtherPeople']\n",
        "\n",
        "    # 画像サイズ\n",
        "    grid_y = crop_size_y / stride\n",
        "    grid_x = crop_size_x / stride\n",
        "    channels = (num_parts + 1) * 2\n",
        "\n",
        "    # 格納する変数\n",
        "    heatmaps = np.zeros((int(grid_y), int(grid_x), 19))\n",
        "    pafs = np.zeros((int(grid_y), int(grid_x), 38))\n",
        "\n",
        "    mask_miss = cv2.resize(mask_miss, (0, 0), fx=1.0 / stride, fy=1.0 /\n",
        "                           stride, interpolation=cv2.INTER_CUBIC).astype(\n",
        "        np.float32)\n",
        "    mask_miss = mask_miss / 255.\n",
        "    mask_miss = np.expand_dims(mask_miss, axis=2)\n",
        "\n",
        "    # マスク変数\n",
        "    heat_mask = np.repeat(mask_miss, 19, axis=2)\n",
        "    paf_mask = np.repeat(mask_miss, 38, axis=2)\n",
        "\n",
        "    # ピンポイントの座標情報をガウス分布にぼやっとさせる\n",
        "    for i in range(18):\n",
        "        if (meta['joint_self'][i, 2] <= 1):\n",
        "            center = meta['joint_self'][i, :2]\n",
        "            gaussian_map = heatmaps[:, :, i]\n",
        "            heatmaps[:, :, i] = putGaussianMaps(\n",
        "                center, gaussian_map, params_transform)\n",
        "        for j in range(nop):\n",
        "            if (meta['joint_others'][j, i, 2] <= 1):\n",
        "                center = meta['joint_others'][j, i, :2]\n",
        "                gaussian_map = heatmaps[:, :, i]\n",
        "                heatmaps[:, :, i] = putGaussianMaps(\n",
        "                    center, gaussian_map, params_transform)\n",
        "    # pafs\n",
        "    mid_1 = [2, 9, 10, 2, 12, 13, 2, 3, 4,\n",
        "             3, 2, 6, 7, 6, 2, 1, 1, 15, 16]\n",
        "\n",
        "    mid_2 = [9, 10, 11, 12, 13, 14, 3, 4, 5,\n",
        "             17, 6, 7, 8, 18, 1, 15, 16, 17, 18]\n",
        "\n",
        "    thre = 1\n",
        "    for i in range(19):\n",
        "        # limb\n",
        "\n",
        "        count = np.zeros((int(grid_y), int(grid_x)), dtype=np.uint32)\n",
        "        if (meta['joint_self'][mid_1[i] - 1, 2] <= 1 and meta['joint_self'][mid_2[i] - 1, 2] <= 1):\n",
        "            centerA = meta['joint_self'][mid_1[i] - 1, :2]\n",
        "            centerB = meta['joint_self'][mid_2[i] - 1, :2]\n",
        "            vec_map = pafs[:, :, 2 * i:2 * i + 2]\n",
        "            #                    print vec_map.shape\n",
        "\n",
        "            pafs[:, :, 2 * i:2 * i + 2], count = putVecMaps(centerA=centerA,\n",
        "                                                            centerB=centerB,\n",
        "                                                            accumulate_vec_map=vec_map,\n",
        "                                                            count=count, params_transform=params_transform)\n",
        "        for j in range(nop):\n",
        "            if (meta['joint_others'][j, mid_1[i] - 1, 2] <= 1 and meta['joint_others'][j, mid_2[i] - 1, 2] <= 1):\n",
        "                centerA = meta['joint_others'][j, mid_1[i] - 1, :2]\n",
        "                centerB = meta['joint_others'][j, mid_2[i] - 1, :2]\n",
        "                vec_map = pafs[:, :, 2 * i:2 * i + 2]\n",
        "                pafs[:, :, 2 * i:2 * i + 2], count = putVecMaps(centerA=centerA,\n",
        "                                                                centerB=centerB,\n",
        "                                                                accumulate_vec_map=vec_map,\n",
        "                                                                count=count, params_transform=params_transform)\n",
        "    # background\n",
        "    heatmaps[:, :, -\n",
        "             1] = np.maximum(1 - np.max(heatmaps[:, :, :18], axis=2), 0.)\n",
        "\n",
        "    # Tensorに\n",
        "    heat_mask = torch.from_numpy(heat_mask)\n",
        "    heatmaps = torch.from_numpy(heatmaps)\n",
        "    paf_mask = torch.from_numpy(paf_mask)\n",
        "    pafs = torch.from_numpy(pafs)\n",
        "\n",
        "    return heat_mask, heatmaps, paf_mask, pafs\n",
        "\n",
        "\n",
        "def make_datapath_list(rootpath):\n",
        "    \"\"\"\n",
        "    学習、検証の画像データとアノテーションデータ、マスクデータへのファイルパスリストを作成する。\n",
        "    \"\"\"\n",
        "\n",
        "    # アノテーションのJSONファイルを読み込む\n",
        "    json_path = os.path.join(rootpath, 'COCO.json')\n",
        "    with open(json_path) as data_file:\n",
        "        data_this = json.load(data_file)\n",
        "        data_json = data_this['root']\n",
        "\n",
        "    # indexを格納\n",
        "    num_samples = len(data_json)\n",
        "    train_indexes = []\n",
        "    val_indexes = []\n",
        "    for count in range(num_samples):\n",
        "        if data_json[count]['isValidation'] != 0.:\n",
        "            val_indexes.append(count)\n",
        "        else:\n",
        "            train_indexes.append(count)\n",
        "\n",
        "    # 画像ファイルパスを格納\n",
        "    train_img_list = list()\n",
        "    val_img_list = list()\n",
        "\n",
        "    for idx in train_indexes:\n",
        "        img_path = os.path.join(rootpath, data_json[idx]['img_paths'])\n",
        "        train_img_list.append(img_path)\n",
        "\n",
        "    for idx in val_indexes:\n",
        "        img_path = os.path.join(rootpath, data_json[idx]['img_paths'])\n",
        "        val_img_list.append(img_path)\n",
        "\n",
        "    # マスクデータのパスを格納\n",
        "    train_mask_list = []\n",
        "    val_mask_list = []\n",
        "\n",
        "    for idx in train_indexes:\n",
        "        img_idx = data_json[idx]['img_paths'][-16:-4]\n",
        "        anno_path = \"/content/pytorch_advanced/4_pose_estimation/data/mask/train2014/mask_COCO_train2014_\" + img_idx+'.jpg'\n",
        "        train_mask_list.append(anno_path)\n",
        "\n",
        "    for idx in val_indexes:\n",
        "        img_idx = data_json[idx]['img_paths'][-16:-4]\n",
        "        anno_path = \"/content/pytorch_advanced/4_pose_estimation/data/mask/val2014/mask_COCO_val2014_\" + img_idx+'.jpg'\n",
        "        val_mask_list.append(anno_path)\n",
        "\n",
        "    # アノテーションデータを格納\n",
        "    train_meta_list = list()\n",
        "    val_meta_list = list()\n",
        "\n",
        "    for idx in train_indexes:\n",
        "        train_meta_list.append(data_json[idx])\n",
        "\n",
        "    for idx in val_indexes:\n",
        "        val_meta_list.append(data_json[idx])\n",
        "\n",
        "    return train_img_list, train_mask_list, val_img_list, val_mask_list, train_meta_list, val_meta_list\n",
        "\n",
        "\n",
        "class DataTransform():\n",
        "    \"\"\"\n",
        "    画像とマスク、アノテーションの前処理クラス。\n",
        "    学習時と推論時で異なる動作をする。\n",
        "    学習時はデータオーギュメンテーションする。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.data_transform = {\n",
        "            'train': Compose([\n",
        "                get_anno(),  # JSONからアノテーションを辞書に格納\n",
        "                add_neck(),  # アノテーションデータの順番を変更し、さらに首のアノテーションデータを追加\n",
        "                aug_scale(),  # 拡大縮小\n",
        "                aug_rotate(),  # 回転\n",
        "                aug_croppad(),  # 切り出し\n",
        "                aug_flip(),  # 左右反転\n",
        "                remove_illegal_joint(),  # 画像からはみ出たアノテーションを除去\n",
        "                Normalize_Tensor()  # 色情報の標準化とテンソル化\n",
        "                # no_Normalize_Tensor()  # 本節のみ、色情報の標準化をなくす\n",
        "            ]),\n",
        "            'val': Compose([\n",
        "                # 本書では検証は省略\n",
        "            ])\n",
        "        }\n",
        "\n",
        "    def __call__(self, phase, meta_data, img, mask_miss):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        phase : 'train' or 'val'\n",
        "            前処理のモードを指定。\n",
        "        \"\"\"\n",
        "        meta_data, img, mask_miss = self.data_transform[phase](\n",
        "            meta_data, img, mask_miss)\n",
        "\n",
        "        return meta_data, img, mask_miss\n",
        "\n",
        "\n",
        "class COCOkeypointsDataset(data.Dataset):\n",
        "    def __init__(self, img_list, mask_list, meta_list, phase, transform):\n",
        "        self.img_list = img_list\n",
        "        self.mask_list = mask_list\n",
        "        self.meta_list = meta_list\n",
        "        self.phase = phase\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        '''画像の枚数を返す'''\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, heatmaps, heat_mask, pafs, paf_mask = self.pull_item(index)\n",
        "        return img, heatmaps, heat_mask, pafs, paf_mask\n",
        "\n",
        "    def pull_item(self, index):\n",
        "        '''画像のTensor形式のデータ、アノテーション、マスクを取得する'''\n",
        "\n",
        "        # 1. 画像読み込み\n",
        "        image_file_path = self.img_list[index]\n",
        "        img = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\n",
        "\n",
        "        # 2. マスクとアノテーション読み込み\n",
        "        mask_miss = cv2.imread(self.mask_list[index])\n",
        "        meat_data = self.meta_list[index]\n",
        "\n",
        "        # 3. 画像前処理\n",
        "        meta_data, img, mask_miss = self.transform(\n",
        "            self.phase, meat_data, img, mask_miss)\n",
        "\n",
        "        # 4. 正解アノテーションデータの取得\n",
        "        mask_miss_numpy = mask_miss.numpy().transpose((1, 2, 0))\n",
        "        heat_mask, heatmaps, paf_mask, pafs = get_ground_truth(\n",
        "            meta_data, mask_miss_numpy)\n",
        "\n",
        "        # 5. マスクデータはRGBが(1,1,1)か(0,0,0)なので、次元を落とす\n",
        "        heat_mask = heat_mask[:, :, :, 0]\n",
        "        paf_mask = paf_mask[:, :, :, 0]\n",
        "\n",
        "        # 6. チャネルが最後尾にあるので順番を変える\n",
        "        # 例：paf_mask：torch.Size([46, 46, 38])\n",
        "        # → torch.Size([38, 46, 46])\n",
        "        paf_mask = paf_mask.permute(2, 0, 1)\n",
        "        heat_mask = heat_mask.permute(2, 0, 1)\n",
        "        pafs = pafs.permute(2, 0, 1)\n",
        "        heatmaps = heatmaps.permute(2, 0, 1)\n",
        "\n",
        "        return img, heatmaps, heat_mask, pafs, paf_mask"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52qy8eDgRTjM"
      },
      "source": [
        "OpenPoseモデル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAWd-oieRRpS"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torchvision"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MyQTAngRZ_T"
      },
      "source": [
        "class OpenPoseNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OpenPoseNet, self).__init__()\n",
        "\n",
        "        # Featureモジュール\n",
        "        self.model0 = OpenPose_Feature()\n",
        "\n",
        "        # Stageモジュール\n",
        "        # PAFs（Part Affinity Fields）側\n",
        "        self.model1_1 = make_OpenPose_block('block1_1')\n",
        "        self.model2_1 = make_OpenPose_block('block2_1')\n",
        "        self.model3_1 = make_OpenPose_block('block3_1')\n",
        "        self.model4_1 = make_OpenPose_block('block4_1')\n",
        "        self.model5_1 = make_OpenPose_block('block5_1')\n",
        "        self.model6_1 = make_OpenPose_block('block6_1')\n",
        "\n",
        "        # confidence heatmap側\n",
        "        self.model1_2 = make_OpenPose_block('block1_2')\n",
        "        self.model2_2 = make_OpenPose_block('block2_2')\n",
        "        self.model3_2 = make_OpenPose_block('block3_2')\n",
        "        self.model4_2 = make_OpenPose_block('block4_2')\n",
        "        self.model5_2 = make_OpenPose_block('block5_2')\n",
        "        self.model6_2 = make_OpenPose_block('block6_2')\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"順伝搬の定義\"\"\"\n",
        "\n",
        "        # Featureモジュール\n",
        "        out1 = self.model0(x)\n",
        "\n",
        "        # Stage1\n",
        "        out1_1 = self.model1_1(out1)  # PAFs側\n",
        "        out1_2 = self.model1_2(out1)  # confidence heatmap側\n",
        "\n",
        "        # CStage2\n",
        "        out2 = torch.cat([out1_1, out1_2, out1], 1)  # 次元1のチャネルで結合\n",
        "        out2_1 = self.model2_1(out2)\n",
        "        out2_2 = self.model2_2(out2)\n",
        "\n",
        "        # Stage3\n",
        "        out3 = torch.cat([out2_1, out2_2, out1], 1)\n",
        "        out3_1 = self.model3_1(out3)\n",
        "        out3_2 = self.model3_2(out3)\n",
        "\n",
        "        # Stage4\n",
        "        out4 = torch.cat([out3_1, out3_2, out1], 1)\n",
        "        out4_1 = self.model4_1(out4)\n",
        "        out4_2 = self.model4_2(out4)\n",
        "\n",
        "        # Stage5\n",
        "        out5 = torch.cat([out4_1, out4_2, out1], 1)\n",
        "        out5_1 = self.model5_1(out5)\n",
        "        out5_2 = self.model5_2(out5)\n",
        "\n",
        "        # Stage6\n",
        "        out6 = torch.cat([out5_1, out5_2, out1], 1)\n",
        "        out6_1 = self.model6_1(out6)\n",
        "        out6_2 = self.model6_2(out6)\n",
        "\n",
        "        # 損失の計算用に各Stageの結果を格納\n",
        "        saved_for_loss = []\n",
        "        saved_for_loss.append(out1_1)  # PAFs側\n",
        "        saved_for_loss.append(out1_2)  # confidence heatmap側\n",
        "        saved_for_loss.append(out2_1)\n",
        "        saved_for_loss.append(out2_2)\n",
        "        saved_for_loss.append(out3_1)\n",
        "        saved_for_loss.append(out3_2)\n",
        "        saved_for_loss.append(out4_1)\n",
        "        saved_for_loss.append(out4_2)\n",
        "        saved_for_loss.append(out5_1)\n",
        "        saved_for_loss.append(out5_2)\n",
        "        saved_for_loss.append(out6_1)\n",
        "        saved_for_loss.append(out6_2)\n",
        "\n",
        "        return (out6_1, out6_2), saved_for_loss"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ess2XY_1Rt-x"
      },
      "source": [
        "Featureモジュール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmx4hyv3Rxl5"
      },
      "source": [
        "class OpenPose_Feature(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OpenPose_Feature, self).__init__()\n",
        "\n",
        "        # VGG-19の最初10個の畳み込みを使用\n",
        "        vgg19 = torchvision.models.vgg19(pretrained=True)\n",
        "        model = {}\n",
        "        model['block0'] = vgg19.features[0:23]  # VGG19の最初の10個の畳み込み層まで\n",
        "\n",
        "        # 残りは新たな畳み込み層を2つ用意\n",
        "        model['block0'].add_module(\"23\", torch.nn.Conv2d(\n",
        "            512, 256, kernel_size=3, stride=1, padding=1))\n",
        "        model['block0'].add_module(\"24\", torch.nn.ReLU(inplace=True))\n",
        "        model['block0'].add_module(\"25\", torch.nn.Conv2d(\n",
        "            256, 128, kernel_size=3, stride=1, padding=1))\n",
        "        model['block0'].add_module(\"26\", torch.nn.ReLU(inplace=True))\n",
        "\n",
        "        self.model = model['block0']\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = self.model(x)\n",
        "        return outputs"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBnlcMyxSCbS"
      },
      "source": [
        "Stageモジュール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYI4d3scSEQE"
      },
      "source": [
        "def make_OpenPose_block(block_name):\n",
        "    # 1. コンフィグレーションの辞書変数blocksを作成し、ネットワークを生成させる\n",
        "    # 最初に全パターンの辞書を用意し、引数block_nameのみを生成する\n",
        "    blocks = {}\n",
        "    # Stage 1\n",
        "    blocks['block1_1'] = [{'conv5_1_CPM_L1': [128, 128, 3, 1, 1]},\n",
        "                          {'conv5_2_CPM_L1': [128, 128, 3, 1, 1]},\n",
        "                          {'conv5_3_CPM_L1': [128, 128, 3, 1, 1]},\n",
        "                          {'conv5_4_CPM_L1': [128, 512, 1, 1, 0]},\n",
        "                          {'conv5_5_CPM_L1': [512, 38, 1, 1, 0]}]\n",
        "\n",
        "    blocks['block1_2'] = [{'conv5_1_CPM_L2': [128, 128, 3, 1, 1]},\n",
        "                          {'conv5_2_CPM_L2': [128, 128, 3, 1, 1]},\n",
        "                          {'conv5_3_CPM_L2': [128, 128, 3, 1, 1]},\n",
        "                          {'conv5_4_CPM_L2': [128, 512, 1, 1, 0]},\n",
        "                          {'conv5_5_CPM_L2': [512, 19, 1, 1, 0]}]\n",
        "\n",
        "    # Stages 2 - 6\n",
        "    for i in range(2, 7):\n",
        "        blocks['block%d_1' % i] = [\n",
        "            {'Mconv1_stage%d_L1' % i: [185, 128, 7, 1, 3]},\n",
        "            {'Mconv2_stage%d_L1' % i: [128, 128, 7, 1, 3]},\n",
        "            {'Mconv3_stage%d_L1' % i: [128, 128, 7, 1, 3]},\n",
        "            {'Mconv4_stage%d_L1' % i: [128, 128, 7, 1, 3]},\n",
        "            {'Mconv5_stage%d_L1' % i: [128, 128, 7, 1, 3]},\n",
        "            {'Mconv6_stage%d_L1' % i: [128, 128, 1, 1, 0]},\n",
        "            {'Mconv7_stage%d_L1' % i: [128, 38, 1, 1, 0]}\n",
        "        ]\n",
        "\n",
        "        blocks['block%d_2' % i] = [\n",
        "            {'Mconv1_stage%d_L2' % i: [185, 128, 7, 1, 3]},\n",
        "            {'Mconv2_stage%d_L2' % i: [128, 128, 7, 1, 3]},\n",
        "            {'Mconv3_stage%d_L2' % i: [128, 128, 7, 1, 3]},\n",
        "            {'Mconv4_stage%d_L2' % i: [128, 128, 7, 1, 3]},\n",
        "            {'Mconv5_stage%d_L2' % i: [128, 128, 7, 1, 3]},\n",
        "            {'Mconv6_stage%d_L2' % i: [128, 128, 1, 1, 0]},\n",
        "            {'Mconv7_stage%d_L2' % i: [128, 19, 1, 1, 0]}\n",
        "        ]\n",
        "\n",
        "    # 引数block_nameのコンフィグレーション辞書を取り出す\n",
        "    cfg_dict = blocks[block_name]\n",
        "\n",
        "    # 2. コンフィグレーション内容をリスト変数layersに格納\n",
        "    layers = []\n",
        "\n",
        "    # 0番目から最後の層までを作成\n",
        "    for i in range(len(cfg_dict)):\n",
        "        for k, v in cfg_dict[i].items():\n",
        "            if 'pool' in k:\n",
        "                layers += [nn.MaxPool2d(kernel_size=v[0], stride=v[1],\n",
        "                                        padding=v[2])]\n",
        "            else:\n",
        "                conv2d = nn.Conv2d(in_channels=v[0], out_channels=v[1],\n",
        "                                   kernel_size=v[2], stride=v[3],\n",
        "                                   padding=v[4])\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "\n",
        "    # 3. layersをSequentialにする\n",
        "    # ただし、最後にReLUはいらないのでその手前までを使用する\n",
        "    net = nn.Sequential(*layers[:-1])\n",
        "\n",
        "    # 4. 初期化関数の設定し、畳み込み層を初期化する\n",
        "\n",
        "    def _initialize_weights_norm(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.normal_(m.weight, std=0.01)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0.0)\n",
        "\n",
        "    net.apply(_initialize_weights_norm)\n",
        "\n",
        "    return net"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywLjEsqLTjQ-"
      },
      "source": [
        "学習と検証"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARscUu_ITeEw"
      },
      "source": [
        "import random\n",
        "import math\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUmns7LyTqsZ"
      },
      "source": [
        "# MS COCOのファイルパスリスト作成\n",
        "train_img_list, train_mask_list, val_img_list, val_mask_list, train_meta_list, val_meta_list = make_datapath_list(\n",
        "    rootpath=\"/content/pytorch_advanced/4_pose_estimation/data/\")\n",
        "\n",
        "# Dataset作成\n",
        "# 本書ではデータ量の問題から、trainをval_listで作成している点に注意\n",
        "train_dataset = COCOkeypointsDataset(\n",
        "    val_img_list, val_mask_list, val_meta_list, phase=\"train\", transform=DataTransform())\n",
        "\n",
        "# 今回は簡易な学習とし検証データは作成しない\n",
        "# val_dataset = CocokeypointsDataset(val_img_list, val_mask_list, val_meta_list, phase=\"val\", transform=DataTransform())\n",
        "\n",
        "# DataLoader作成\n",
        "batch_size = 32\n",
        "\n",
        "train_dataloader = data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# val_dataloader = data.DataLoader(\n",
        "#    val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 辞書型変数にまとめる\n",
        "# dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
        "dataloaders_dict = {\"train\": train_dataloader, \"val\": None}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhHVNBO7Tv79"
      },
      "source": [
        "class OpenPoseLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OpenPoseLoss, self).__init__()\n",
        "\n",
        "    def forward(self, saved_for_loss, heatmap_target, heat_mask, paf_target, paf_mask):\n",
        "        total_loss = 0\n",
        "        # ステージごとに計算します\n",
        "        for j in range(6):\n",
        "\n",
        "            # PAFsとheatmapsにおいて、マスクされている部分（paf_mask=0など）は無視させる\n",
        "            # PAFs\n",
        "            pred1 = saved_for_loss[2 * j] * paf_mask\n",
        "            gt1 = paf_target.float() * paf_mask\n",
        "\n",
        "            # heatmaps\n",
        "            pred2 = saved_for_loss[2 * j + 1] * heat_mask\n",
        "            gt2 = heatmap_target.float()*heat_mask\n",
        "\n",
        "            total_loss += F.mse_loss(pred1, gt1, reduction='mean') + \\\n",
        "                F.mse_loss(pred2, gt2, reduction='mean')\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "\n",
        "criterion = OpenPoseLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9cXbneST_Z3"
      },
      "source": [
        "optimizer = optim.SGD(net.parameters(), lr=1e-2, momentum=0.9, weight_decay=0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWRI1AtrUGgX"
      },
      "source": [
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "\n",
        "    # GPUが使えるかを確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス：\", device)\n",
        "\n",
        "    # ネットワークをGPUへ\n",
        "    net.to(device)\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # 画像の枚数\n",
        "    num_train_imgs = len(dataloaders_dict[\"train\"].dataset)\n",
        "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
        "\n",
        "    # イテレーションカウンタをセット\n",
        "    iteration = 1\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # 開始時刻を保存\n",
        "        t_epoch_start = time.time()\n",
        "        t_iter_start = time.time()\n",
        "        epoch_train_loss = 0.0  # epochの損失和\n",
        "        epoch_val_loss = 0.0  # epochの損失和\n",
        "\n",
        "        print('-------------')\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-------------')\n",
        "\n",
        "        # epochごとの訓練と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()  # モデルを訓練モードに\n",
        "                optimizer.zero_grad()\n",
        "                print('（train）')\n",
        "\n",
        "            # 今回は検証はスキップ\n",
        "            else:\n",
        "                continue\n",
        "                # net.eval()   # モデルを検証モードに\n",
        "                # print('-------------')\n",
        "                # print('（val）')\n",
        "\n",
        "            # データローダーからminibatchずつ取り出すループ\n",
        "            for imges, heatmap_target, heat_mask, paf_target, paf_mask in dataloaders_dict[phase]:\n",
        "                # ミニバッチがサイズが1だと、バッチノーマライゼーションでエラーになるのでさける\n",
        "                if imges.size()[0] == 1:\n",
        "                    continue\n",
        "\n",
        "                # GPUが使えるならGPUにデータを送る\n",
        "                imges = imges.to(device)\n",
        "                heatmap_target = heatmap_target.to(device)\n",
        "                heat_mask = heat_mask.to(device)\n",
        "                paf_target = paf_target.to(device)\n",
        "                paf_mask = paf_mask.to(device)\n",
        "\n",
        "                # optimizerを初期化\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # (out6_1, out6_2)は使わないので _ で代替\n",
        "                    _, saved_for_loss = net(imges)\n",
        "\n",
        "                    loss = criterion(saved_for_loss, heatmap_target,\n",
        "                                     heat_mask, paf_target, paf_mask)\n",
        "                    del saved_for_loss\n",
        "                    # 訓練時はバックプロパゲーション\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
        "                            t_iter_finish = time.time()\n",
        "                            duration = t_iter_finish - t_iter_start\n",
        "                            print('イテレーション {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
        "                                iteration, loss.item()/batch_size, duration))\n",
        "                            t_iter_start = time.time()\n",
        "\n",
        "                        epoch_train_loss += loss.item()\n",
        "                        iteration += 1\n",
        "                    # 検証時\n",
        "                    # else:\n",
        "                        #epoch_val_loss += loss.item()\n",
        "\n",
        "        # epochのphaseごとのlossと正解率\n",
        "        t_epoch_finish = time.time()\n",
        "        print('-------------')\n",
        "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
        "            epoch+1, epoch_train_loss/num_train_imgs, 0))\n",
        "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
        "        t_epoch_start = time.time()\n",
        "\n",
        "    # 最後のネットワークを保存する\n",
        "    torch.save(net.state_dict(), 'weights/openpose_net_' + str(epoch+1) + '.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6V6i7Q_U5yh"
      },
      "source": [
        "推論"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrvIFToaU1A8"
      },
      "source": [
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtGp3bz-VAW3"
      },
      "source": [
        "net = OpenPoseNet()\n",
        "\n",
        "# 学習済みパラメータをロードする\n",
        "net_weights = torch.load(\n",
        "    '/content/pytorch_advanced/4_pose_estimation/weights/pose_model_scratch.pth', map_location={'cuda:0': 'cpu'})\n",
        "keys = list(net_weights.keys())\n",
        "\n",
        "weights_load = {}\n",
        "\n",
        "# ロードした内容を、本書で構築したモデルの\n",
        "# パラメータ名net.state_dict().keys()にコピーする\n",
        "for i in range(len(keys)):\n",
        "    weights_load[list(net.state_dict().keys())[i]] = net_weights[list(keys)[i]]\n",
        "\n",
        "# コピーした内容をモデルに与える\n",
        "state = net.state_dict()\n",
        "state.update(weights_load)\n",
        "net.load_state_dict(state)\n",
        "\n",
        "print('ネットワーク設定完了：学習済みの重みをロードしました')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72N-WdlnVVC5"
      },
      "source": [
        "test_image = '/content/pytorch_advanced/4_pose_estimation/data/hit-1407826_640.jpg'\n",
        "oriImg = cv2.imread(test_image)  # B,G,Rの順番\n",
        "\n",
        "# BGRをRGBにして表示\n",
        "oriImg = cv2.cvtColor(oriImg, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(oriImg)\n",
        "plt.show()\n",
        "\n",
        "# 画像のリサイズ\n",
        "size = (368, 368)\n",
        "img = cv2.resize(oriImg, size, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "# 画像の前処理\n",
        "img = img.astype(np.float32) / 255.\n",
        "\n",
        "# 色情報の標準化\n",
        "color_mean = [0.485, 0.456, 0.406]\n",
        "color_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "# 21/03/07 Issue147 https://github.com/YutaroOgawa/pytorch_advanced/issues/147\n",
        "# 色チャネルの順番を誤っています\n",
        "# preprocessed_img = img.copy()[:, :, ::-1]  # RGB→BGR\n",
        "preprocessed_img = img.copy()  # RGB\n",
        "\n",
        "for i in range(3):\n",
        "    preprocessed_img[:, :, i] = preprocessed_img[:, :, i] - color_mean[i]\n",
        "    preprocessed_img[:, :, i] = preprocessed_img[:, :, i] / color_std[i]\n",
        "\n",
        "# （高さ、幅、色）→（色、高さ、幅）\n",
        "img = preprocessed_img.transpose((2, 0, 1)).astype(np.float32)\n",
        "\n",
        "# 画像をTensorに\n",
        "img = torch.from_numpy(img)\n",
        "\n",
        "# ミニバッチ化：torch.Size([1, 3, 368, 368])\n",
        "x = img.unsqueeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ubm_gOxVpRh"
      },
      "source": [
        "# OpenPoseでheatmapsとPAFsを求めます\n",
        "net.eval()\n",
        "predicted_outputs, _ = net(x)\n",
        "\n",
        "# 画像をテンソルからNumPyに変化し、サイズを戻します\n",
        "pafs = predicted_outputs[0][0].detach().numpy().transpose(1, 2, 0)\n",
        "heatmaps = predicted_outputs[1][0].detach().numpy().transpose(1, 2, 0)\n",
        "\n",
        "pafs = cv2.resize(pafs, size, interpolation=cv2.INTER_CUBIC)\n",
        "heatmaps = cv2.resize(heatmaps, size, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "pafs = cv2.resize(\n",
        "    pafs, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
        "heatmaps = cv2.resize(\n",
        "    heatmaps, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv2.INTER_CUBIC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX6nyTcpVuqA"
      },
      "source": [
        "import math\n",
        "\n",
        "import cv2\n",
        "import matplotlib.cm\n",
        "import numpy as np\n",
        "from scipy.ndimage.filters import gaussian_filter, maximum_filter\n",
        "from scipy.ndimage.morphology import generate_binary_structure\n",
        "\n",
        "# It is better to use 0.1 as threshold when evaluation, but 0.3 for demo\n",
        "# purpose.\n",
        "cmap = matplotlib.cm.get_cmap('hsv')\n",
        "\n",
        "# Heatmap indices to find each limb (joint connection). Eg: limb_type=1 is\n",
        "# Neck->LShoulder, so joint_to_limb_heatmap_relationship[1] represents the\n",
        "# indices of heatmaps to look for joints: neck=1, LShoulder=5\n",
        "joint_to_limb_heatmap_relationship = [\n",
        "    [1, 2], [1, 5], [2, 3], [3, 4], [5, 6], [6, 7], [1, 8], [8, 9], [9, 10],\n",
        "    [1, 11], [11, 12], [12, 13], [1, 0], [0, 14], [14, 16], [0, 15], [15, 17],\n",
        "    [2, 16], [5, 17]]\n",
        "\n",
        "# PAF indices containing the x and y coordinates of the PAF for a given limb.\n",
        "# Eg: limb_type=1 is Neck->LShoulder, so\n",
        "# PAFneckLShoulder_x=paf_xy_coords_per_limb[1][0] and\n",
        "# PAFneckLShoulder_y=paf_xy_coords_per_limb[1][1]\n",
        "paf_xy_coords_per_limb = [\n",
        "    [12, 13], [20, 21], [14, 15], [16, 17], [22, 23],\n",
        "    [24, 25], [0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [28, 29],\n",
        "    [30, 31], [34, 35], [32, 33], [36, 37], [18, 19], [26, 27]]\n",
        "\n",
        "# Color code used to plot different joints and limbs (eg: joint_type=3 and\n",
        "# limb_type=3 will use colors[3])\n",
        "colors = [\n",
        "    [255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0],\n",
        "    [85, 255, 0], [0, 255, 0], [0, 255, 85], [0, 255, 170], [0, 255, 255],\n",
        "    [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255], [170, 0, 255],\n",
        "    [255, 0, 255], [255, 0, 170], [255, 0, 85], [255, 0, 0]]\n",
        "\n",
        "NUM_JOINTS = 18\n",
        "NUM_LIMBS = len(joint_to_limb_heatmap_relationship)\n",
        "\n",
        "\n",
        "def find_peaks(param, img):\n",
        "    \"\"\"\n",
        "    Given a (grayscale) image, find local maxima whose value is above a given\n",
        "    threshold (param['thre1'])\n",
        "    :param img: Input image (2d array) where we want to find peaks\n",
        "    :return: 2d np.array containing the [x,y] coordinates of each peak found\n",
        "    in the image\n",
        "    \"\"\"\n",
        "\n",
        "    peaks_binary = (maximum_filter(img, footprint=generate_binary_structure(\n",
        "        2, 1)) == img) * (img > param['thre1'])\n",
        "    # Note reverse ([::-1]): we return [[x y], [x y]...] instead of [[y x], [y\n",
        "    # x]...]\n",
        "    return np.array(np.nonzero(peaks_binary)[::-1]).T\n",
        "\n",
        "\n",
        "def compute_resized_coords(coords, resizeFactor):\n",
        "    \"\"\"\n",
        "    Given the index/coordinates of a cell in some input array (e.g. image),\n",
        "    provides the new coordinates if that array was resized by making it\n",
        "    resizeFactor times bigger.\n",
        "    E.g.: image of size 3x3 is resized to 6x6 (resizeFactor=2), we'd like to\n",
        "    know the new coordinates of cell [1,2] -> Function would return [2.5,4.5]\n",
        "    :param coords: Coordinates (indices) of a cell in some input array\n",
        "    :param resizeFactor: Resize coefficient = shape_dest/shape_source. E.g.:\n",
        "    resizeFactor=2 means the destination array is twice as big as the\n",
        "    original one\n",
        "    :return: Coordinates in an array of size\n",
        "    shape_dest=resizeFactor*shape_source, expressing the array indices of the\n",
        "    closest point to 'coords' if an image of size shape_source was resized to\n",
        "    shape_dest\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Add 0.5 to coords to get coordinates of center of the pixel (e.g.\n",
        "    # index [0,0] represents the pixel at location [0.5,0.5])\n",
        "    # 2) Transform those coordinates to shape_dest, by multiplying by resizeFactor\n",
        "    # 3) That number represents the location of the pixel center in the new array,\n",
        "    # so subtract 0.5 to get coordinates of the array index/indices (revert\n",
        "    # step 1)\n",
        "    return (np.array(coords, dtype=float) + 0.5) * resizeFactor - 0.5\n",
        "\n",
        "\n",
        "def NMS(param, heatmaps, upsampFactor=1., bool_refine_center=True, bool_gaussian_filt=False):\n",
        "    \"\"\"\n",
        "    NonMaximaSuppression: find peaks (local maxima) in a set of grayscale images\n",
        "    :param heatmaps: set of grayscale images on which to find local maxima (3d np.array,\n",
        "    with dimensions image_height x image_width x num_heatmaps)\n",
        "    :param upsampFactor: Size ratio between CPM heatmap output and the input image size.\n",
        "    Eg: upsampFactor=16 if original image was 480x640 and heatmaps are 30x40xN\n",
        "    :param bool_refine_center: Flag indicating whether:\n",
        "     - False: Simply return the low-res peak found upscaled by upsampFactor (subject to grid-snap)\n",
        "     - True: (Recommended, very accurate) Upsample a small patch around each low-res peak and\n",
        "     fine-tune the location of the peak at the resolution of the original input image\n",
        "    :param bool_gaussian_filt: Flag indicating whether to apply a 1d-GaussianFilter (smoothing)\n",
        "    to each upsampled patch before fine-tuning the location of each peak.\n",
        "    :return: a NUM_JOINTS x 4 np.array where each row represents a joint type (0=nose, 1=neck...)\n",
        "    and the columns indicate the {x,y} position, the score (probability) and a unique id (counter)\n",
        "    \"\"\"\n",
        "    # MODIFIED BY CARLOS: Instead of upsampling the heatmaps to heatmap_avg and\n",
        "    # then performing NMS to find peaks, this step can be sped up by ~25-50x by:\n",
        "    # (9-10ms [with GaussFilt] or 5-6ms [without GaussFilt] vs 250-280ms on RoG\n",
        "    # 1. Perform NMS at (low-res) CPM's output resolution\n",
        "    # 1.1. Find peaks using scipy.ndimage.filters.maximum_filter\n",
        "    # 2. Once a peak is found, take a patch of 5x5 centered around the peak, upsample it, and\n",
        "    # fine-tune the position of the actual maximum.\n",
        "    #  '-> That's equivalent to having found the peak on heatmap_avg, but much faster because we only\n",
        "    #      upsample and scan the 5x5 patch instead of the full (e.g.) 480x640\n",
        "\n",
        "    joint_list_per_joint_type = []\n",
        "    cnt_total_joints = 0\n",
        "\n",
        "    # For every peak found, win_size specifies how many pixels in each\n",
        "    # direction from the peak we take to obtain the patch that will be\n",
        "    # upsampled. Eg: win_size=1 -> patch is 3x3; win_size=2 -> 5x5\n",
        "    # (for BICUBIC interpolation to be accurate, win_size needs to be >=2!)\n",
        "    win_size = 2\n",
        "\n",
        "    for joint in range(NUM_JOINTS):\n",
        "        map_orig = heatmaps[:, :, joint]\n",
        "        peak_coords = find_peaks(param, map_orig)\n",
        "        peaks = np.zeros((len(peak_coords), 4))\n",
        "        for i, peak in enumerate(peak_coords):\n",
        "            if bool_refine_center:\n",
        "                x_min, y_min = np.maximum(0, peak - win_size)\n",
        "                x_max, y_max = np.minimum(\n",
        "                    np.array(map_orig.T.shape) - 1, peak + win_size)\n",
        "\n",
        "                # Take a small patch around each peak and only upsample that\n",
        "                # tiny region\n",
        "                patch = map_orig[y_min:y_max + 1, x_min:x_max + 1]\n",
        "                map_upsamp = cv2.resize(\n",
        "                    patch, None, fx=upsampFactor, fy=upsampFactor, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "                # Gaussian filtering takes an average of 0.8ms/peak (and there might be\n",
        "                # more than one peak per joint!) -> For now, skip it (it's\n",
        "                # accurate enough)\n",
        "                map_upsamp = gaussian_filter(\n",
        "                    map_upsamp, sigma=3) if bool_gaussian_filt else map_upsamp\n",
        "\n",
        "                # Obtain the coordinates of the maximum value in the patch\n",
        "                location_of_max = np.unravel_index(\n",
        "                    map_upsamp.argmax(), map_upsamp.shape)\n",
        "                # Remember that peaks indicates [x,y] -> need to reverse it for\n",
        "                # [y,x]\n",
        "                location_of_patch_center = compute_resized_coords(\n",
        "                    peak[::-1] - [y_min, x_min], upsampFactor)\n",
        "                # Calculate the offset wrt to the patch center where the actual\n",
        "                # maximum is\n",
        "                refined_center = (location_of_max - location_of_patch_center)\n",
        "                peak_score = map_upsamp[location_of_max]\n",
        "            else:\n",
        "                refined_center = [0, 0]\n",
        "                # Flip peak coordinates since they are [x,y] instead of [y,x]\n",
        "                peak_score = map_orig[tuple(peak[::-1])]\n",
        "            peaks[i, :] = tuple([int(round(x)) for x in compute_resized_coords(\n",
        "                peak_coords[i], upsampFactor) + refined_center[::-1]]) + (peak_score, cnt_total_joints)\n",
        "            cnt_total_joints += 1\n",
        "        joint_list_per_joint_type.append(peaks)\n",
        "\n",
        "    return joint_list_per_joint_type\n",
        "\n",
        "\n",
        "def find_connected_joints(param, paf_upsamp, joint_list_per_joint_type, num_intermed_pts=10):\n",
        "    \"\"\"\n",
        "    For every type of limb (eg: forearm, shin, etc.), look for every potential\n",
        "    pair of joints (eg: every wrist-elbow combination) and evaluate the PAFs to\n",
        "    determine which pairs are indeed body limbs.\n",
        "    :param paf_upsamp: PAFs upsampled to the original input image resolution\n",
        "    :param joint_list_per_joint_type: See 'return' doc of NMS()\n",
        "    :param num_intermed_pts: Int indicating how many intermediate points to take\n",
        "    between joint_src and joint_dst, at which the PAFs will be evaluated\n",
        "    :return: List of NUM_LIMBS rows. For every limb_type (a row) we store\n",
        "    a list of all limbs of that type found (eg: all the right forearms).\n",
        "    For each limb (each item in connected_limbs[limb_type]), we store 5 cells:\n",
        "    # {joint_src_id,joint_dst_id}: a unique number associated with each joint,\n",
        "    # limb_score_penalizing_long_dist: a score of how good a connection\n",
        "    of the joints is, penalized if the limb length is too long\n",
        "    # {joint_src_index,joint_dst_index}: the index of the joint within\n",
        "    all the joints of that type found (eg: the 3rd right elbow found)\n",
        "    \"\"\"\n",
        "    connected_limbs = []\n",
        "\n",
        "    # Auxiliary array to access paf_upsamp quickly\n",
        "    limb_intermed_coords = np.empty((4, num_intermed_pts), dtype=np.intp)\n",
        "    for limb_type in range(NUM_LIMBS):\n",
        "        # List of all joints of type A found, where A is specified by limb_type\n",
        "        # (eg: a right forearm starts in a right elbow)\n",
        "        joints_src = joint_list_per_joint_type[joint_to_limb_heatmap_relationship[limb_type][0]]\n",
        "        # List of all joints of type B found, where B is specified by limb_type\n",
        "        # (eg: a right forearm ends in a right wrist)\n",
        "        joints_dst = joint_list_per_joint_type[joint_to_limb_heatmap_relationship[limb_type][1]]\n",
        "        if len(joints_src) == 0 or len(joints_dst) == 0:\n",
        "            # No limbs of this type found (eg: no right forearms found because\n",
        "            # we didn't find any right wrists or right elbows)\n",
        "            connected_limbs.append([])\n",
        "        else:\n",
        "            connection_candidates = []\n",
        "            # Specify the paf index that contains the x-coord of the paf for\n",
        "            # this limb\n",
        "            limb_intermed_coords[2, :] = paf_xy_coords_per_limb[limb_type][0]\n",
        "            # And the y-coord paf index\n",
        "            limb_intermed_coords[3, :] = paf_xy_coords_per_limb[limb_type][1]\n",
        "            for i, joint_src in enumerate(joints_src):\n",
        "                # Try every possible joints_src[i]-joints_dst[j] pair and see\n",
        "                # if it's a feasible limb\n",
        "                for j, joint_dst in enumerate(joints_dst):\n",
        "                    # Subtract the position of both joints to obtain the\n",
        "                    # direction of the potential limb\n",
        "                    limb_dir = joint_dst[:2] - joint_src[:2]\n",
        "                    # Compute the distance/length of the potential limb (norm\n",
        "                    # of limb_dir)\n",
        "                    limb_dist = np.sqrt(np.sum(limb_dir**2)) + 1e-8\n",
        "                    limb_dir = limb_dir / limb_dist  # Normalize limb_dir to be a unit vector\n",
        "\n",
        "                    # Linearly distribute num_intermed_pts points from the x\n",
        "                    # coordinate of joint_src to the x coordinate of joint_dst\n",
        "                    limb_intermed_coords[1, :] = np.round(np.linspace(\n",
        "                        joint_src[0], joint_dst[0], num=num_intermed_pts))\n",
        "                    limb_intermed_coords[0, :] = np.round(np.linspace(\n",
        "                        joint_src[1], joint_dst[1], num=num_intermed_pts))  # Same for the y coordinate\n",
        "                    intermed_paf = paf_upsamp[limb_intermed_coords[0, :],\n",
        "                                              limb_intermed_coords[1, :], limb_intermed_coords[2:4, :]].T\n",
        "\n",
        "                    score_intermed_pts = intermed_paf.dot(limb_dir)\n",
        "                    score_penalizing_long_dist = score_intermed_pts.mean(\n",
        "                    ) + min(0.5 * paf_upsamp.shape[0] / limb_dist - 1, 0)\n",
        "                    # Criterion 1: At least 80% of the intermediate points have\n",
        "                    # a score higher than thre2\n",
        "                    criterion1 = (np.count_nonzero(\n",
        "                        score_intermed_pts > param['thre2']) > 0.8 * num_intermed_pts)\n",
        "                    # Criterion 2: Mean score, penalized for large limb\n",
        "                    # distances (larger than half the image height), is\n",
        "                    # positive\n",
        "                    criterion2 = (score_penalizing_long_dist > 0)\n",
        "                    if criterion1 and criterion2:\n",
        "                        # Last value is the combined paf(+limb_dist) + heatmap\n",
        "                        # scores of both joints\n",
        "                        connection_candidates.append(\n",
        "                            [i, j, score_penalizing_long_dist, score_penalizing_long_dist + joint_src[2] + joint_dst[2]])\n",
        "\n",
        "            # Sort connection candidates based on their\n",
        "            # score_penalizing_long_dist\n",
        "            connection_candidates = sorted(\n",
        "                connection_candidates, key=lambda x: x[2], reverse=True)\n",
        "            connections = np.empty((0, 5))\n",
        "            # There can only be as many limbs as the smallest number of source\n",
        "            # or destination joints (eg: only 2 forearms if there's 5 wrists\n",
        "            # but 2 elbows)\n",
        "            max_connections = min(len(joints_src), len(joints_dst))\n",
        "            # Traverse all potential joint connections (sorted by their score)\n",
        "            for potential_connection in connection_candidates:\n",
        "                i, j, s = potential_connection[0:3]\n",
        "                # Make sure joints_src[i] or joints_dst[j] haven't already been\n",
        "                # connected to other joints_dst or joints_src\n",
        "                if i not in connections[:, 3] and j not in connections[:, 4]:\n",
        "                    # [joint_src_id, joint_dst_id, limb_score_penalizing_long_dist, joint_src_index, joint_dst_index]\n",
        "                    connections = np.vstack(\n",
        "                        [connections, [joints_src[i][3], joints_dst[j][3], s, i, j]])\n",
        "                    # Exit if we've already established max_connections\n",
        "                    # connections (each joint can't be connected to more than\n",
        "                    # one joint)\n",
        "                    if len(connections) >= max_connections:\n",
        "                        break\n",
        "            connected_limbs.append(connections)\n",
        "\n",
        "    return connected_limbs\n",
        "\n",
        "\n",
        "def group_limbs_of_same_person(connected_limbs, joint_list):\n",
        "    \"\"\"\n",
        "    Associate limbs belonging to the same person together.\n",
        "    :param connected_limbs: See 'return' doc of find_connected_joints()\n",
        "    :param joint_list: unravel'd version of joint_list_per_joint [See 'return' doc of NMS()]\n",
        "    :return: 2d np.array of size num_people x (NUM_JOINTS+2). For each person found:\n",
        "    # First NUM_JOINTS columns contain the index (in joint_list) of the joints associated\n",
        "    with that person (or -1 if their i-th joint wasn't found)\n",
        "    # 2nd-to-last column: Overall score of the joints+limbs that belong to this person\n",
        "    # Last column: Total count of joints found for this person\n",
        "    \"\"\"\n",
        "    person_to_joint_assoc = []\n",
        "\n",
        "    for limb_type in range(NUM_LIMBS):\n",
        "        joint_src_type, joint_dst_type = joint_to_limb_heatmap_relationship[limb_type]\n",
        "\n",
        "        for limb_info in connected_limbs[limb_type]:\n",
        "            person_assoc_idx = []\n",
        "            for person, person_limbs in enumerate(person_to_joint_assoc):\n",
        "                if person_limbs[joint_src_type] == limb_info[0] or person_limbs[joint_dst_type] == limb_info[1]:\n",
        "                    person_assoc_idx.append(person)\n",
        "\n",
        "            # If one of the joints has been associated to a person, and either\n",
        "            # the other joint is also associated with the same person or not\n",
        "            # associated to anyone yet:\n",
        "            if len(person_assoc_idx) == 1:\n",
        "                person_limbs = person_to_joint_assoc[person_assoc_idx[0]]\n",
        "                # If the other joint is not associated to anyone yet,\n",
        "                if person_limbs[joint_dst_type] != limb_info[1]:\n",
        "                    # Associate it with the current person\n",
        "                    person_limbs[joint_dst_type] = limb_info[1]\n",
        "                    # Increase the number of limbs associated to this person\n",
        "                    person_limbs[-1] += 1\n",
        "                    # And update the total score (+= heatmap score of joint_dst\n",
        "                    # + score of connecting joint_src with joint_dst)\n",
        "                    person_limbs[-2] += joint_list[limb_info[1]\n",
        "                                                   .astype(int), 2] + limb_info[2]\n",
        "            elif len(person_assoc_idx) == 2:  # if found 2 and disjoint, merge them\n",
        "                person1_limbs = person_to_joint_assoc[person_assoc_idx[0]]\n",
        "                person2_limbs = person_to_joint_assoc[person_assoc_idx[1]]\n",
        "                membership = ((person1_limbs >= 0) & (person2_limbs >= 0))[:-2]\n",
        "                if not membership.any():  # If both people have no same joints connected, merge them into a single person\n",
        "                    # Update which joints are connected\n",
        "                    person1_limbs[:-2] += (person2_limbs[:-2] + 1)\n",
        "                    # Update the overall score and total count of joints\n",
        "                    # connected by summing their counters\n",
        "                    person1_limbs[-2:] += person2_limbs[-2:]\n",
        "                    # Add the score of the current joint connection to the\n",
        "                    # overall score\n",
        "                    person1_limbs[-2] += limb_info[2]\n",
        "                    person_to_joint_assoc.pop(person_assoc_idx[1])\n",
        "                else:  # Same case as len(person_assoc_idx)==1 above\n",
        "                    person1_limbs[joint_dst_type] = limb_info[1]\n",
        "                    person1_limbs[-1] += 1\n",
        "                    person1_limbs[-2] += joint_list[limb_info[1]\n",
        "                                                    .astype(int), 2] + limb_info[2]\n",
        "            else:  # No person has claimed any of these joints, create a new person\n",
        "                # Initialize person info to all -1 (no joint associations)\n",
        "                row = -1 * np.ones(20)\n",
        "                # Store the joint info of the new connection\n",
        "                row[joint_src_type] = limb_info[0]\n",
        "                row[joint_dst_type] = limb_info[1]\n",
        "                # Total count of connected joints for this person: 2\n",
        "                row[-1] = 2\n",
        "                # Compute overall score: score joint_src + score joint_dst + score connection\n",
        "                # {joint_src,joint_dst}\n",
        "                row[-2] = sum(joint_list[limb_info[:2].astype(int), 2]\n",
        "                              ) + limb_info[2]\n",
        "                person_to_joint_assoc.append(row)\n",
        "\n",
        "    # Delete people who have very few parts connected\n",
        "    people_to_delete = []\n",
        "    for person_id, person_info in enumerate(person_to_joint_assoc):\n",
        "        if person_info[-1] < 3 or person_info[-2] / person_info[-1] < 0.2:\n",
        "            people_to_delete.append(person_id)\n",
        "    # Traverse the list in reverse order so we delete indices starting from the\n",
        "    # last one (otherwise, removing item for example 0 would modify the indices of\n",
        "    # the remaining people to be deleted!)\n",
        "    for index in people_to_delete[::-1]:\n",
        "        person_to_joint_assoc.pop(index)\n",
        "\n",
        "    # Appending items to a np.array can be very costly (allocating new memory, copying over the array, then adding new row)\n",
        "    # Instead, we treat the set of people as a list (fast to append items) and\n",
        "    # only convert to np.array at the end\n",
        "    return np.array(person_to_joint_assoc)\n",
        "\n",
        "\n",
        "def plot_pose(img_orig, joint_list, person_to_joint_assoc, bool_fast_plot=True, plot_ear_to_shoulder=False):\n",
        "    canvas = img_orig.copy()  # Make a copy so we don't modify the original image\n",
        "\n",
        "    # to_plot is the location of all joints found overlaid on top of the\n",
        "    # original image\n",
        "    to_plot = canvas.copy() if bool_fast_plot else cv2.addWeighted(\n",
        "        img_orig, 0.3, canvas, 0.7, 0)\n",
        "\n",
        "    limb_thickness = 4\n",
        "    # Last 2 limbs connect ears with shoulders and this looks very weird.\n",
        "    # Disabled by default to be consistent with original rtpose output\n",
        "    which_limbs_to_plot = NUM_LIMBS if plot_ear_to_shoulder else NUM_LIMBS - 2\n",
        "    for limb_type in range(which_limbs_to_plot):\n",
        "        for person_joint_info in person_to_joint_assoc:\n",
        "            joint_indices = person_joint_info[joint_to_limb_heatmap_relationship[limb_type]].astype(\n",
        "                int)\n",
        "            if -1 in joint_indices:\n",
        "                # Only draw actual limbs (connected joints), skip if not\n",
        "                # connected\n",
        "                continue\n",
        "            # joint_coords[:,0] represents Y coords of both joints;\n",
        "            # joint_coords[:,1], X coords\n",
        "            joint_coords = joint_list[joint_indices, 0:2]\n",
        "\n",
        "            for joint in joint_coords:  # Draw circles at every joint\n",
        "                cv2.circle(canvas, tuple(joint[0:2].astype(\n",
        "                    int)), 4, (255, 255, 255), thickness=-1)\n",
        "            # mean along the axis=0 computes meanYcoord and meanXcoord -> Round\n",
        "            # and make int to avoid errors\n",
        "            coords_center = tuple(\n",
        "                np.round(np.mean(joint_coords, 0)).astype(int))\n",
        "            # joint_coords[0,:] is the coords of joint_src; joint_coords[1,:]\n",
        "            # is the coords of joint_dst\n",
        "            limb_dir = joint_coords[0, :] - joint_coords[1, :]\n",
        "            limb_length = np.linalg.norm(limb_dir)\n",
        "            # Get the angle of limb_dir in degrees using atan2(limb_dir_x,\n",
        "            # limb_dir_y)\n",
        "            angle = math.degrees(math.atan2(limb_dir[1], limb_dir[0]))\n",
        "\n",
        "            # For faster plotting, just plot over canvas instead of constantly\n",
        "            # copying it\n",
        "            cur_canvas = canvas if bool_fast_plot else canvas.copy()\n",
        "            polygon = cv2.ellipse2Poly(\n",
        "                coords_center, (int(limb_length / 2), limb_thickness), int(angle), 0, 360, 1)\n",
        "            cv2.fillConvexPoly(cur_canvas, polygon, colors[limb_type])\n",
        "            if not bool_fast_plot:\n",
        "                canvas = cv2.addWeighted(canvas, 0.4, cur_canvas, 0.6, 0)\n",
        "\n",
        "    return to_plot, canvas\n",
        "\n",
        "\n",
        "def decode_pose(img_orig, heatmaps, pafs):\n",
        "    param = {'thre1': 0.1, 'thre2': 0.05, 'thre3': 0.5}\n",
        "\n",
        "    # Bottom-up approach:\n",
        "    # Step 1: find all joints in the image (organized by joint type: [0]=nose,\n",
        "    # [1]=neck...)\n",
        "    joint_list_per_joint_type = NMS(param,\n",
        "                                    heatmaps, img_orig.shape[0] / float(heatmaps.shape[0]))\n",
        "    # joint_list is an unravel'd version of joint_list_per_joint, where we add\n",
        "    # a 5th column to indicate the joint_type (0=nose, 1=neck...)\n",
        "    joint_list = np.array([tuple(peak) + (joint_type,) for joint_type,\n",
        "                           joint_peaks in enumerate(joint_list_per_joint_type) for peak in joint_peaks])\n",
        "\n",
        "    # Step 2: find which joints go together to form limbs (which wrists go\n",
        "    # with which elbows)\n",
        "    paf_upsamp = cv2.resize(\n",
        "        pafs, (img_orig.shape[1], img_orig.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
        "    connected_limbs = find_connected_joints(param,\n",
        "                                            paf_upsamp, joint_list_per_joint_type)\n",
        "\n",
        "    # Step 3: associate limbs that belong to the same person\n",
        "    person_to_joint_assoc = group_limbs_of_same_person(\n",
        "        connected_limbs, joint_list)\n",
        "\n",
        "    # (Step 4): plot results\n",
        "    to_plot, canvas = plot_pose(img_orig, joint_list, person_to_joint_assoc)\n",
        "\n",
        "    return to_plot, canvas, joint_list, person_to_joint_assoc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtwwYEaoWClb"
      },
      "source": [
        "_, result_img, _, _ = decode_pose(oriImg, heatmaps, pafs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikdAjbe4WHsM"
      },
      "source": [
        "plt.imshow(oriImg)\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(result_img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}